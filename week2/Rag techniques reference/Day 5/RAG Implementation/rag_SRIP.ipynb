{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk3pd-CPCGbH"
      },
      "source": [
        "# Steps to build a RAG from scratch\n",
        "### Step-1: Import ``` ChatTogether ``` from ``` langchain_together ```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain-together"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH871C7rD4Jq",
        "outputId": "001cdbf5-1bf5-4fcc-cb59-709152d256b2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-together\n",
            "  Downloading langchain_together-0.3.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from langchain-together) (3.11.15)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.11/dist-packages (from langchain-together) (0.3.59)\n",
            "Collecting langchain-openai<0.4,>=0.3 (from langchain-together)\n",
            "  Downloading langchain_openai-0.3.18-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-together) (2.32.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-together) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-together) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-together) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-together) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-together) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-together) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-together) (1.20.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-together) (0.3.42)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-together) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-together) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-together) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-together) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-together) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-together) (2.11.4)\n",
            "Collecting langchain-core<0.4.0,>=0.3.29 (from langchain-together)\n",
            "  Downloading langchain_core-0.3.61-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai<0.4,>=0.3->langchain-together) (1.78.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai<0.4,>=0.3->langchain-together) (0.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-together) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-together) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-together) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-together) (2025.4.26)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain-together) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain-together) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain-together) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain-together) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain-together) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai<0.4,>=0.3->langchain-together) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai<0.4,>=0.3->langchain-together) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai<0.4,>=0.3->langchain-together) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai<0.4,>=0.3->langchain-together) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai<0.4,>=0.3->langchain-together) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.29->langchain-together) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.29->langchain-together) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.29->langchain-together) (0.4.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai<0.4,>=0.3->langchain-together) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain-together) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain-together) (0.16.0)\n",
            "Downloading langchain_together-0.3.0-py3-none-any.whl (12 kB)\n",
            "Downloading langchain_openai-0.3.18-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.61-py3-none-any.whl (438 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.3/438.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-core, langchain-openai, langchain-together\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.59\n",
            "    Uninstalling langchain-core-0.3.59:\n",
            "      Successfully uninstalled langchain-core-0.3.59\n",
            "Successfully installed langchain-core-0.3.61 langchain-openai-0.3.18 langchain-together-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vDuxzVD2CGbJ"
      },
      "outputs": [],
      "source": [
        "from langchain_together import ChatTogether"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zICFWqjoCGbJ"
      },
      "outputs": [],
      "source": [
        "api_key=\"API_Key\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNVlwpk_CGbJ"
      },
      "source": [
        "### Step-3: Define your model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Pm6lvvNFVxw",
        "outputId": "18695704-72ab-4373-b2f5-22ab0e07ec32"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.61)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.42)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (2.11.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain_community-0.3.24 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJxhw0QtFn65",
        "outputId": "08f7b369-f780-4a4a-f021-3d683e122be1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.5.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Downloading pypdf-5.5.0-py3-none-any.whl (303 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/303.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/303.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEyaf58VCGbL"
      },
      "source": [
        "### Step-6: Load the pdf for context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b34dRP9CGbL",
        "outputId": "468e9dd4-b1af-43e4-9a60-2c86ccb27e96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 7 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 9 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 17 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 19 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 21 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 23 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 29 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 35 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 37 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 43 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 45 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 47 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 49 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 51 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 53 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 55 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 62 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 64 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 66 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 68 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 70 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 76 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 80 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 86 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 88 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 95 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 97 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240813034150Z00'00'\", 'moddate': \"D:20240813034150Z00'00'\", 'source': 'paper.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Machine LearningLogistic Regression DR. BHARGA VI RSCOPEVIT CHENNAI'),\n",
              " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240813034150Z00'00'\", 'moddate': \"D:20240813034150Z00'00'\", 'source': 'paper.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='Bhargavi RClassification - ApplicationsBinary Classification • Online transactions – Fraudulent / Not Fraudulent• Email – Spam/ Not spam ?• Tumor classification – Malignant/BenignMulti-class Classification• Optical Character Recognition• Face classification Multi-Label ClassificationA variant of theclassificationproblem where multiple nonexclusive labels may be assigned to each instance.'),\n",
              " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240813034150Z00'00'\", 'moddate': \"D:20240813034150Z00'00'\", 'source': 'paper.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Bhargavi RBinary classification 𝑦∈0,1 : 0 - Negative class (Not spam)                   : 1 – Positive class (Spam)'),\n",
              " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240813034150Z00'00'\", 'moddate': \"D:20240813034150Z00'00'\", 'source': 'paper.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='Bhargavi RLogistic Regression - Introduction• Linear model.• Used for binary classification• Can be extended to handle multiclass as well• Computationally inexpensive• Easy to implement.• Logistic Regression models the response/prediction as probability that y (output variable) belongs to a particular category.\\nx1\\nx2'),\n",
              " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240813034150Z00'00'\", 'moddate': \"D:20240813034150Z00'00'\", 'source': 'paper.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='Bhargavi RHypothesis Function• Can the hypothesis function  gx,𝑤=∑!\"#$𝑤!𝑥! be used for classification?• gx,𝑤\\tresults in a real value (–∞ <g(w,x)< +∞). • For classification problems we need the result to be finite discrete values representing different classes.•𝑆𝑖𝑔𝑚𝑜𝑖𝑑𝑥=##%&!\"•  ℎ\\'𝑥=##%\\t&!#(\",&)= ##%\\t&!∑)*+,&)\")\\t• h𝑤x   can also be written as ##%\\t&!\\t&./• ℎ\\'𝑥 is called as Sigmoid/ logistic function • 0≤ℎ\\'𝑥≤1'),\n",
              " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240813034150Z00'00'\", 'moddate': \"D:20240813034150Z00'00'\", 'source': 'paper.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='Bhargavi RHypothesis Function (cont...)•  Value of  hw(x) is the estimated probability that y =1, for input x with given w’s• h𝑤x = P(y=1 /x; w) i.e probability that y =1, for input x with given w’s• And P(y=0 /x; w) = 1 - P(y=1 /x; w) (since P(y=1 /x; w) + P(y=0 /x; w) = 1)• If\\tℎ!𝑥≥0.5\\ti.e 𝑤\"𝑥≥0\\tthen y = 1 • If\\tℎ!𝑥<0.5\\ti.e 𝑤\"𝑥<0\\tthen y = 0• For fixed w’s, 𝒘𝑻𝒙 represent a linear decision boundary • xi’s that results in 𝑤\"𝑥 ≥0 are predicted as 1• xi’s that results in 𝑤\"𝑥 <0 are predicted as 0\\n𝑤0𝑥\\ny = 1y = 0'),\n",
              " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240813034150Z00'00'\", 'moddate': \"D:20240813034150Z00'00'\", 'source': 'paper.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='Bhargavi RCost Function •Cost should be minimum (≈ 0) for the correct predictions and maximum(≈ ∞ or very high value) for the wrong predictions.For a single observation : Cost (hw(x), y) =  -log(hw(x)) for y = 1 (i.e for +ve sample)                     -log(1 - hw(x)) for y = 0 (i.e for -ve sample)\\n-log(hw(x)) plot -log(1-hw(x)) plot'),\n",
              " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240813034150Z00'00'\", 'moddate': \"D:20240813034150Z00'00'\", 'source': 'paper.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='Bhargavi RCost Function (cont…)• Combining the cost for positive and negative predictions into a single equationCost (hw(x), y) = −𝑦𝑙𝑜𝑔ℎ!𝑥−1−𝑦log1−\\tℎ!𝑥• Cost function for n data points can be written as               J(w)  =&\\'∑()&\\'𝑐𝑜𝑠𝑡(ℎ!(𝒙(,𝑦())                            =&\\'∑()&\\'−𝑦(𝑙𝑜𝑔ℎ!𝑥(−(1−𝑦()log(1−\\tℎ!𝑥()                           = −&\\'∑()&\\'𝑦(𝑙𝑜𝑔ℎ!𝑥(+(1−𝑦()log(1−\\tℎ!𝑥()Where ℎ!𝒙(=&&*\\t,!\"#𝒙𝒊'),\n",
              " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240813034150Z00'00'\", 'moddate': \"D:20240813034150Z00'00'\", 'source': 'paper.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='Bhargavi RGradient of Cost FunctionGradient of J(w):Derivative of hypothesis function !\"&!#  = $%%&\\t(!𝒘#𝒙()\\t×𝑒$𝒘#𝒙(×(−𝒙+)  = %%&\\t(!𝒘#𝒙((!𝒘#𝒙(\\t%&\\t(!𝒘#𝒙( 𝒙+ = hw(1 – hw)xiSubstituting partial derivative of the hypothesis in the cost function and simplifying we getGradient of J(w) (say w.r.t wj) = %,∑+-%,(ℎ#(𝒙+)−\\t𝑦+)𝑥+.'),\n",
              " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240813034150Z00'00'\", 'moddate': \"D:20240813034150Z00'00'\", 'source': 'paper.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='Bhargavi RGradient DescentDo repeatedly{  𝑤*=\\t𝑤*−\\t𝛼++,1(𝐽(𝑊))}orDo repeatedly{  𝑤*=\\t𝑤*−\\t𝛼\\t∑!\"#-(ℎ\\'(𝒙!)−\\t𝑦!)𝑥!*    (Simultaneously update all Wj’s)}Here 𝛼 is the learning rate'),\n",
              " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240813034150Z00'00'\", 'moddate': \"D:20240813034150Z00'00'\", 'source': 'paper.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='Bhargavi RPredictionThe marketing department of a credit card company wants to organize a campaign to convince existing holders of the company’s standard credit card to upgrade to the company’s premium card for a nominal annual fee. The marketing department begins with the question “Which of the existing standard credit cardholders should be the target for the campaign?”Dataset - 30 cardholders data that indicates whether the cardholder upgraded to a premium or not (y i.e response)Two independent variables/features: 1.Total amount of credit card purchases in the prior year(x1) 2.Whether the cardholder ordered additional credit cards (at extra cost) for other members of the household ( x2 : 0 no, 1 yes).The regression coefficient vales are w0  = -6.940, w1 = 0.13947, w2 = 2.774'),\n",
              " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240813034150Z00'00'\", 'moddate': \"D:20240813034150Z00'00'\", 'source': 'paper.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='Bhargavi RPrediction (cont…)Consider a cardholder who charged $36,000 last year and possesses additional cards for members of the household. What is the probability the cardholder will upgrade to the premium card during the marketing campaign.Substitute the wis and xis in the function ℎ#𝒙+=%%&\\t(!\"#𝒙𝒊 to the predicted probability.-6.94 + (0.13947)(36) + (2.774)(1) = 0.85492e-(0.85492) = 0.423Estimated probability of purchasing premium card = 1 / (1 + 0.423) = 0.702'),\n",
              " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240813034150Z00'00'\", 'moddate': \"D:20240813034150Z00'00'\", 'source': 'paper.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='Bhargavi RMulticlass Classification𝑦∈1,2,3,….\\nx1\\nx2'),\n",
              " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240813034150Z00'00'\", 'moddate': \"D:20240813034150Z00'00'\", 'source': 'paper.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='Bhargavi RMulticlass Classification (cont…)One-vs-all (or) one-vs-rest\\nx1\\nx2'),\n",
              " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240813034150Z00'00'\", 'moddate': \"D:20240813034150Z00'00'\", 'source': 'paper.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='Bhargavi RMulticlass Classification (cont…)One-vs-all (or) one-vs-rest : Step1 : Modify the training data such that only one specific class has y=1 and rest all have y=0.Step 2: Train the classifier.Step3: Repeat Step 1 and 2 for remaining all classes each time making one class as y=1 and remaining all as y=0 and training individual models.Step 4: Prediction : For a new test input xt pick a class that maximizes the hw(xt)')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader=PyPDFLoader(\"paper.pdf\")\n",
        "pages=loader.load_and_split()\n",
        "pages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1200,\n",
        "    chunk_overlap=100,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
        ")\n",
        "page_split = text_splitter.split_documents(pages)"
      ],
      "metadata": {
        "id": "7grM_CTBKw0s"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOD37RLxCGbL"
      },
      "source": [
        "### Step-8: Create a vector store with the embeddings and context pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri6lcaplCGbL"
      },
      "source": [
        "> #### DocArrayInMemorySearch Explanation\n",
        ">\n",
        "> This code creates a simple in-memory vector store that:\n",
        "> - Converts document pages into vector embeddings\n",
        "> - Stores them in memory for quick similarity searches\n",
        "> - Useful for testing and prototyping RAG applications\n",
        "> - Data is temporary and cleared when program ends"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install docarray"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pNB0YOGF02g",
        "outputId": "e7c3c566-4d89-4cd7-bf61-6649f7984dc8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting docarray\n",
            "  Downloading docarray-0.41.0-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from docarray) (2.0.2)\n",
            "Requirement already satisfied: orjson>=3.8.2 in /usr/local/lib/python3.11/dist-packages (from docarray) (3.10.18)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from docarray) (2.11.4)\n",
            "Requirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.11/dist-packages (from docarray) (13.9.4)\n",
            "Collecting types-requests>=2.28.11.6 (from docarray)\n",
            "  Downloading types_requests-2.32.0.20250515-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from docarray) (0.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.8->docarray) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.8->docarray) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.8->docarray) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.8->docarray) (0.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->docarray) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->docarray) (2.19.1)\n",
            "Requirement already satisfied: urllib3>=2 in /usr/local/lib/python3.11/dist-packages (from types-requests>=2.28.11.6->docarray) (2.4.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->docarray) (1.1.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray) (0.1.2)\n",
            "Downloading docarray-0.41.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.0.20250515-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: types-requests, docarray\n",
            "Successfully installed docarray-0.41.0 types-requests-2.32.0.20250515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "\n",
        "embedding_function = SentenceTransformerEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528,
          "referenced_widgets": [
            "169d32e1e2374e30b3f49aea5c31eb5b",
            "c56b471d5db84145965f23d71965d905",
            "a32870fe2cef4de4b451a108d3312c31",
            "25879ef600e146058f1ba006ca319510",
            "52d696cf397e448b892e1ec47bde5d7e",
            "671ac300d02b4843bcefa54387cd52be",
            "f2720aaa4cc54a0da94650ec0af53cf5",
            "1689669297cd4d5fb333e5c9bd178c06",
            "5890bc1b6f524a7792962b2a95cd88ff",
            "ec64225aaa54457398e99e50b97959bf",
            "d26534653f3e463b928ad88605de6bf6",
            "dbef60af59ca41a397fa9f0d1fb3e345",
            "c2c804de574c4b189c27e99acb33637e",
            "d42b8789def54eb6beb49f68c3d20ce6",
            "5bb71cc7374143828ca5d9945b8b4a5a",
            "8cd75f9da49348b78640299e78ae410b",
            "a225e4556e83413487ea17b753497013",
            "d1b99d1e209142ac842cf420d2c98967",
            "1ccdf81da6e24339989b58d5011e7d60",
            "a0a4b9360b0a42728b5078000461f249",
            "f0d2bf6a8f374edface751cfe8c4cbba",
            "8cc5b2294b2b471c993829b6f3e0d33b",
            "09d6d9002a9644f0acad1ea521b191d4",
            "1e32cf93fa1d4c319dc6dc8239980e11",
            "a3e761d09e44453a94740cef92746ebd",
            "d6071e83cf1c47909811fe7ad718eadf",
            "d3eade9599004ec9ac018fe5a4f2aef5",
            "4c8ff67f019b40e6843c974473453bc3",
            "ae786c582fe545c4956284c27edb23fb",
            "76cb181b205a415b88ee94bebfcba0e9",
            "a23c75f2b42d42e3b95ce12fe72d740a",
            "0bc6960a25224b99afeea0362548fe0c",
            "e042fa9021ea4de8acc3c4c368477985",
            "77d86724afa24245888d9b090024107e",
            "80488676dcbb494bbcf4faa239a12681",
            "3fa6e4f704ad40c7ba27030b50a6c62e",
            "b37f285664d741dca97c655dc40d489f",
            "13c4a66546e6459d8593981f9a3e4f30",
            "5f3cbce7d987415aa29fb282e7976762",
            "67fbd275cb5c47e8a669e6d6d54ee449",
            "0c3ab34e49f4487ab5595461e5f8f48a",
            "e07c1a6edaf3438fa4798e315e86a782",
            "5413c2c459c2445ca31cff4631c27d35",
            "b405e20920b24f48b4c831e8e6987687",
            "b8b95fadac574f03a7a36c88cb1085e0",
            "da75015aecd044d7b4035077ada6c05b",
            "fe6f174458ac49dabd2fc81def7869af",
            "b6b5a1f798a44ea7b2ffcffdb5206b1f",
            "ae7139f8ca254de6ad3e3e1089b3e14e",
            "a4ffe32b2b7f47da9c72c2d3084d90e9",
            "d1b821f8c68644cd8efd241273cf5514",
            "fda6d4dc40ce404c9b7c0dff593c7619",
            "b30606f6ac4841d49d712429f4cd7351",
            "cdbf443c167a4bd280723c1f008e1947",
            "164c54c1c903456cad3fe9dbfde52d2c",
            "0d5edc94566c43ebaf82a2845f064584",
            "64253b8addd4427286f3dbc6f6bf1f2e",
            "48966039760445a4a22bd680f90816f4",
            "f42e5e9dde4b44439db29e42586e4536",
            "dbd2aac6352941b38d02f7e9e15447e5",
            "79d1192e29764020bf2048976b895c83",
            "fbf687b576c545128d1089f4c2fd3f3b",
            "0b6c6406e2fc47d38d74ed38e0e14caa",
            "6adccbcebe584d28ab03211f78504d02",
            "3e949b65cd7c4b72bf60035cc2a550f8",
            "2cff5472964845d0ade105c01db8cd3b",
            "ab2421b2b0944792a3a0c44f87070962",
            "9d13799042f94a5ea537c6fcb1cb2df9",
            "b3061a5ecabb425d85e789a91baa40a9",
            "e7fd88ef2b364b3c9d992bfc710312b4",
            "e46274912a934daaa72c6282f5bbee2d",
            "cfd8dd90e5d94c90bbe4db03fd0ecba9",
            "f88b6de8d0514ba9bfa0710d66a79448",
            "79557332e85646d8a83ec2c250e7e7e2",
            "8d0c62013505413b9b08ea96911ebf98",
            "df83648ec4c34690885457d20caff082",
            "2fcbffafda6b42f19d110d631cdbe46b",
            "2f4addd80f054b20a71082231c7935c2",
            "aa77b89197924e8885372ed1eecd2d6e",
            "00cd01bd7d954784bc8e4e3b03b0ec27",
            "ae86b2e9f0f241bcb992cf4ce1d5acd6",
            "db7b5d971e4e4585adc7242d9ba48f0c",
            "4a7b777c975a41e5b4a835281c107321",
            "da8269fbd7d24eebade7ad05904ddb44",
            "61a7ee33581548db8a5081bea39ccbee",
            "5490d51f6b0e43ca9660d8965167917b",
            "ce434bbc29004427952c1d8c493ed132",
            "bcd0cd5d0de3406780c4128fbfef1126",
            "058b7b53e3024257b281efff8a0c02ce",
            "e0b534e87e054c4ab234bdffbade062a",
            "6ed8868054b344f8a75baf107f2cbbcc",
            "e38d66c3df0e4beea8502de2cf326274",
            "4137b248354c48cb8382c0c02f4e4e8b",
            "a87ec6b77bb840689e77ce5b94af6adc",
            "46cd03ce161f40d4bcebe1ddad7683f4",
            "0c7b64f111654130905828818ab3da34",
            "d70c1c84f8e041c39e08b41c21fbeb95",
            "16f556540399443bbefe032aae7de0ab",
            "54f15c75510a4e33b9d133b8d99e3a91",
            "b35a1209807640e284041918d2b60252",
            "7fcda9face474f85b3980c06cb9297a0",
            "7c0117876eb34b8a95db214ca9ca7311",
            "3a554b41f2e24136b6fac8112de264af",
            "f8e9b55f7d11457199d57cadf699ce3a",
            "a3f63b2ede1f407893e9aa50a5b7eeeb",
            "fff9d760ec284cbaa537ae39614d3756",
            "bd1b5ea1e07248eebfdbc68118bc00f3",
            "d57bc8b1bdcb49719ee4a31b3cb680cd",
            "70bcd78ff42f4b7ab1899b0d77ff64f5",
            "ddeb8aaf0866467493d9de8fe7e69bc5",
            "e085fe2b4b7740a88380bea6e56de746",
            "8ba8e37b335f4de8a638b98aa991d15a",
            "a606f86d95e84aeeb8c7144d8505ca8b",
            "e04120a768504259b3d540c91e69f3cb",
            "d9580191c17a459fbdd04c9ceb7d7a9d",
            "acbb5c59b0b54085917286cb9fee5a37",
            "a0be87b6b58c41898713304faaf08f8c",
            "537a078aef4b46eaa78b9c3fd2fe1960",
            "6ea679c764cf4df3828677019e980212",
            "374c5b48b7724910b886fab2a699abff",
            "35a2c334dcbc4605816e65f588cc823a"
          ]
        },
        "id": "bEiY667AGNr2",
        "outputId": "9a11099e-1c81-46db-a236-e001dfc8d130"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-e4a653ae12bb>:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_function = SentenceTransformerEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "169d32e1e2374e30b3f49aea5c31eb5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbef60af59ca41a397fa9f0d1fb3e345"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/94.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09d6d9002a9644f0acad1ea521b191d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77d86724afa24245888d9b090024107e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8b95fadac574f03a7a36c88cb1085e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d5edc94566c43ebaf82a2845f064584"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab2421b2b0944792a3a0c44f87070962"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f4addd80f054b20a71082231c7935c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "058b7b53e3024257b281efff8a0c02ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b35a1209807640e284041918d2b60252"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e085fe2b4b7740a88380bea6e56de746"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HTakBCMdCGbL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6b48d25-9836-44e1-d882-acd5a00a6e5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
            "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
        "\n",
        "vectorstore = DocArrayInMemorySearch.from_documents(\n",
        "    page_split,\n",
        "    embedding=embedding_function\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80Xo2oskCGbL"
      },
      "source": [
        "### Step-9: Defining the retriever\n",
        "\n",
        "> #### Retriever Operations\n",
        "> This code:\n",
        "> - Creates a retriever from the vector store using `as_retriever()`\n",
        "> - Uses `invoke()` to search for documents related to \"Machine Learning\"\n",
        "> - Returns semantically similar content from the stored documents\n",
        ">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIgusnF7CGbM",
        "outputId": "381eefe4-c086-43ef-de1c-14dfa794684e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240813034150Z00'00'\", 'moddate': \"D:20240813034150Z00'00'\", 'source': 'paper.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Machine LearningLogistic Regression DR. BHARGA VI RSCOPEVIT CHENNAI'),\n",
              " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240813034150Z00'00'\", 'moddate': \"D:20240813034150Z00'00'\", 'source': 'paper.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='Bhargavi RLogistic Regression - Introduction• Linear model.• Used for binary classification• Can be extended to handle multiclass as well• Computationally inexpensive• Easy to implement.• Logistic Regression models the response/prediction as probability that y (output variable) belongs to a particular category.\\nx1\\nx2'),\n",
              " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240813034150Z00'00'\", 'moddate': \"D:20240813034150Z00'00'\", 'source': 'paper.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='Bhargavi RGradient DescentDo repeatedly{  𝑤*=\\t𝑤*−\\t𝛼++,1(𝐽(𝑊))}orDo repeatedly{  𝑤*=\\t𝑤*−\\t𝛼\\t∑!\"#-(ℎ\\'(𝒙!)−\\t𝑦!)𝑥!*    (Simultaneously update all Wj’s)}Here 𝛼 is the learning rate'),\n",
              " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20240813034150Z00'00'\", 'moddate': \"D:20240813034150Z00'00'\", 'source': 'paper.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='Bhargavi RMulticlass Classification (cont…)One-vs-all (or) one-vs-rest : Step1 : Modify the training data such that only one specific class has y=1 and rest all have y=0.Step 2: Train the classifier.Step3: Repeat Step 1 and 2 for remaining all classes each time making one class as y=1 and remaining all as y=0 and training individual models.Step 4: Prediction : For a new test input xt pick a class that maximizes the hw(xt)')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "retriever=vectorstore.as_retriever()\n",
        "\n",
        "retriever.invoke(\"Machine Learning\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template=\"\"\"\n",
        "    Answer the question based on the context below. If you don't know the answer, just say so.\n",
        "    Context: {context}\n",
        "    Question: {question}\n",
        "\"\"\"\n",
        "prompt=PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "-UVPEWvqMkjm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\"\n",
        "\n",
        "model = ChatTogether(\n",
        "    together_api_key=api_key,\n",
        "    model=MODEL,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "9G23r_7NMucI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8PCntvoCGbM"
      },
      "source": [
        "### Step-10: Defining the chain for the RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kdQstBlwCGbM"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "chain = (\n",
        "    {\n",
        "        \"context\":itemgetter(\"question\") | retriever,\n",
        "        \"question\":itemgetter(\"question\")\n",
        "    }\n",
        "    | prompt\n",
        "    | model\n",
        "    | parser\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "MvYydwsNMz-k"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "JlEUtnN_CGbM",
        "outputId": "4152bbbe-0681-437c-b446-42c46d5ad5fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The PDF appears to be about machine learning and data analysis, specifically focusing on topics such as binary classification, logistic regression, and gradient descent, with examples related to credit card marketing and customer behavior.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "chain.invoke({\"question\":\"What is the pdf about?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvzT6fRNCGbM",
        "outputId": "094f8a57-78e3-473a-c4e6-66b334854410"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, I can generate a quiz with 15 questions. Here are the questions:\n",
            "\n",
            "1. What is the topic discussed on page 12 of the document?\n",
            "A) Gradient Descent\n",
            "B) Multiclass Classification\n",
            "C) Prediction\n",
            "D) Regression\n",
            "\n",
            "Answer: B) Multiclass Classification\n",
            "\n",
            "2. What is the formula for updating the weights in Gradient Descent?\n",
            "A) w* = w* - α * (J(W))\n",
            "B) w* = w* + α * (J(W))\n",
            "C) w* = w* - α * ∑(h'(x) - y) * x\n",
            "D) w* = w* + α * ∑(h'(x) - y) * x\n",
            "\n",
            "Answer: A) w* = w* - α * (J(W))\n",
            "\n",
            "3. What is the learning rate in Gradient Descent?\n",
            "A) α\n",
            "B) β\n",
            "C) γ\n",
            "D) δ\n",
            "\n",
            "Answer: A) α\n",
            "\n",
            "4. What is the goal of the marketing department in the given scenario?\n",
            "A) To convince existing holders of the company's premium credit card to upgrade to the standard card\n",
            "B) To convince existing holders of the company's standard credit card to upgrade to the premium card\n",
            "C) To convince new customers to apply for the company's credit card\n",
            "D) To convince existing holders of the company's credit card to cancel their accounts\n",
            "\n",
            "Answer: B) To convince existing holders of the company's standard credit card to upgrade to the premium card\n",
            "\n",
            "5. What are the two independent variables/features used in the regression model?\n",
            "A) Total amount of credit card purchases and whether the cardholder ordered additional credit cards\n",
            "B) Total amount of credit card purchases and the cardholder's age\n",
            "C) Whether the cardholder ordered additional credit cards and the cardholder's income\n",
            "D) Total amount of credit card purchases and the cardholder's occupation\n",
            "\n",
            "Answer: A) Total amount of credit card purchases and whether the cardholder ordered additional credit cards\n",
            "\n",
            "6. What are the values of the regression coefficients w0, w1, and w2?\n",
            "A) w0 = -6.940, w1 = 0.13947, w2 = 2.774\n",
            "B) w0 = -6.940, w1 = 0.13947, w2 = 1.774\n",
            "C) w0 = -5.940, w1 = 0.13947, w2 = 2.774\n",
            "D) w0 = -7.940, w1 = 0.13947, w2 = 2.774\n",
            "\n",
            "Answer: A) w0 = -6.940, w1 = 0.13947, w2 = 2.774\n",
            "\n",
            "7. What is the technique used for multiclass classification?\n",
            "A) One-vs-all (or) one-vs-rest\n",
            "B) One-vs-one\n",
            "C) Multi-layer perceptron\n",
            "D) Decision tree\n",
            "\n",
            "Answer: A) One-vs-all (or) one-vs-rest\n",
            "\n",
            "8. How many steps are involved in the one-vs-all technique?\n",
            "A) 2\n",
            "B) 3\n",
            "C) 4\n",
            "D) 5\n",
            "\n",
            "Answer: C) 4\n",
            "\n",
            "9. What is the first step in the one-vs-all technique?\n",
            "A) Train the classifier\n",
            "B) Modify the training data\n",
            "C) Predict the output\n",
            "D) Evaluate the model\n",
            "\n",
            "Answer: B) Modify the training data\n",
            "\n",
            "10. What is the purpose of modifying the training data in the one-vs-all technique?\n",
            "A) To increase the accuracy of the model\n",
            "B) To decrease the complexity of the model\n",
            "C) To make one specific class have y=1 and rest all have y=0\n",
            "D) To make all classes have y=1\n",
            "\n",
            "Answer: C) To make one specific class have y=1 and rest all have y=0\n",
            "\n",
            "11. How many individual models are trained in the one-vs-all technique?\n",
            "A) 1\n",
            "B) 2\n",
            "C) Equal to the number of classes\n",
            "D) Equal to the number of features\n",
            "\n",
            "Answer: C) Equal to the number of classes\n",
            "\n",
            "12. What is the purpose of training individual models in the one-vs-all technique?\n",
            "A) To increase the accuracy of the model\n",
            "B) To decrease the complexity of the model\n",
            "C) To make predictions for each class\n",
            "D) To evaluate the model\n",
            "\n",
            "Answer: C) To make predictions for each class\n",
            "\n",
            "13. How is the prediction made in the one-vs-all technique?\n",
            "A) By picking the class that maximizes the hw(x)\n",
            "B) By picking the class that minimizes the hw(x)\n",
            "C) By picking the class that has the highest probability\n",
            "D) By picking the class that has the lowest probability\n",
            "\n",
            "Answer: A) By picking the class that maximizes the hw(x)\n",
            "\n",
            "14. What is the advantage of using the one-vs-all technique?\n",
            "A) It can handle only binary classification problems\n",
            "B) It can handle only multi-class classification problems\n",
            "C) It can handle both binary and multi-class classification problems\n",
            "D) It is more complex than other techniques\n",
            "\n",
            "Answer: C) It can handle both binary and multi-class classification problems\n",
            "\n",
            "15. What is the disadvantage of using the one-vs-all technique?\n",
            "A) It is more accurate than other techniques\n",
            "B) It is less complex than other techniques\n",
            "C) It requires training multiple models\n",
            "D) It is faster than other techniques\n",
            "\n",
            "Answer: C) It requires training multiple models\n",
            "\n",
            "Note: Please keep in mind that these questions are based on the provided context and may not be comprehensive or exhaustive.\n"
          ]
        }
      ],
      "source": [
        "quizText=chain.invoke({\"question\":\"Can you generate a quiz of 15 questions based on the provided notes ?\"})\n",
        "print(quizText)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "svVDvIZjCGbM",
        "outputId": "177e6963-4e7e-4063-9282-dc533839351a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 0, 'page_label': '1'}, page_content='Chapter I: Introduction to LLMs'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 1, 'page_label': '2'}, page_content='What are Large Language Models\\nBy now, you might have heard of them. Large Language Models,\\ncommonly known as LLMs, are a sophisticated type of neural network.\\nThese models ignited many innovations in the field of natural language\\nprocessing (NLP) and are characterized by their large number of\\nparameters, often in billions, that make them proficient at processing and\\ngenerating text. They are trained on extensive textual data, enabling them to\\ngrasp various language patterns and structures. The primary goal of LLMs\\nis to interpret and create human-like text that captures the nuances of\\nnatural language, including syntax (the arrangement of words) and\\nsemantics (the meaning of words).\\nThe core training objective of LLMs focuses on predicting the next word in\\na sentence. This straightforward objective leads to the development of\\nemergent abilities. For example, they can conduct arithmetic calculations,\\nunscramble words, and have even demonstrated proficiency in professional\\nexams, such as passing the US Medical Licensing Exam. Additionally, these\\nmodels have significantly contributed to various NLP tasks, including\\nmachine translation, natural language generation, part-of-speech tagging,\\nparsing, information retrieval, and others, even without direct training or\\nfine-tuning in these specific areas.\\nThe text generation process in Large Language Models is autoregressive,\\nmeaning they generate the next tokens based on the sequence of tokens\\nalready generated. The attention mechanism is a vital component in this\\nprocess; it establishes word connections and ensures the text is coherent and\\ncontextually appropriate. It is essential to establish the fundamental\\nterminology and concepts associated with Large Language Models before\\nexploring the architecture and its building blocks (like attention\\nmechanisms) in greater depth. Let’s start with an overview of the\\narchitecture that powers these models, followed by defining a few terms,\\nsuch as language modeling and tokenization.'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 2, 'page_label': '3'}, page_content='Key LLM Terminologies\\nThe Transformer\\nThe foundation of a language model that makes it powerful lies in its\\narchitecture. Recurrent Neural Networks (RNNs) were traditionally used for\\ntext processing due to their ability to process sequential data. They maintain\\nan internal state that retains information from previous words, facilitating\\nsequential understanding. However, RNNs encounter challenges with long\\nsequences where they forget older information in favor of recently\\nprocessed input. This is primarily caused by the vanishing gradient\\nproblem, a phenomenon where the gradients, which are used to update the\\nnetwork’s weights during training, become increasingly smaller as they are\\npropagated back through each timestep of the sequence. As a result, the\\nweights associated with early inputs change very little, hindering the\\nnetwork’s ability to learn from and remember long-term dependencies\\nwithin the data.\\nTransformer-based models addressed these challenges and emerged as the\\npreferred architecture for natural language processing tasks. This\\narchitecture introduced in the influential paper “Attention Is All You Need”\\nis a pivotal innovation in natural language processing. It forms the\\nfoundation for cutting-edge models like GPT-4, Claude, and LLaMA. The\\narchitecture was originally designed as an encoder-decoder framework. This\\nsetting uses an encoder to process input text, identifying important parts and\\ncreating a representation of the input. Meanwhile, the decoder is capable of\\ntransforming the encoder’s output, a vector of high dimensionality, back\\ninto readable text for humans. These networks can be useful in tasks such as\\nsummarization, where the decoder generates summaries conditioned based\\non the articles passed to the encoder. It offers additional flexibility across a\\nwide range of tasks since the components of this architecture, the encoder,\\nand decoder, can be used jointly or independently. Some models use the\\nencoder part of the network to transform the text into a vector\\nrepresentation or use only the decoder block, which is the backbone of the\\nLarge Language Models. The next chapter will cover each of these\\ncomponents.'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 3, 'page_label': '4'}, page_content='Language Modeling\\nWith the rise of LLMs, language modeling has become an essential part of\\nnatural language processing. It means learning the probability distribution\\nof words within a language based on a large corpus. This learning process\\ntypically involves predicting the next token in a sequence using either\\nclassical statistical methods or novel deep learning techniques.\\nLarge language models are trained based on the same objective to predict\\nthe next word, punctuation mark, or other elements based on the seen\\ntokens in a text. These models become proficient by understanding the\\ndistribution of words within their training data by guessing the probability\\nof the next word based on the context. For example, the model can\\ncomplete a sentence beginning with “I live in New” with a word like\\n“York” rather than an unrelated word such as “shoe”.\\nIn practice, the models work with tokens, not complete words. This\\napproach allows for more accurate predictions and text generation by more\\neffectively capturing the complexity of human language.\\nTokenization\\nTokenization is the initial phase of interacting with LLMs. It involves\\nbreaking down the input text into smaller pieces known as tokens. Tokens\\ncan range from single characters to entire words, and the size of these\\ntokens can greatly influence the model’s performance. Some models adopt\\nsubword tokenization, breaking words into smaller segments that retain\\nmeaningful linguistic elements.\\nConsider the following sentence, “The child’s coloring book.”\\nIf tokenization splits the text after every white space character. The result\\nwill be:\\n[\"The\", \"child\\'s\", “coloring”, \"book.\"]'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 4, 'page_label': '5'}, page_content='In this approach, you’ll notice that the punctuation remains attached to the\\nwords like “child’s” and “book.”\\nAlternatively, tokenization can be done by separating text based on both\\nwhite spaces and punctuation; the output would be:\\n[\"The\", \"child\", \"\\'\", \"s\", “coloring”, \"book\", \".\"]\\nThe tokenization process is model-dependent. It’s important to remember\\nthat the models are released as a pair of pre-trained tokenizers and\\nassociated model weights. There are more advanced techniques, like the\\nByte-Pair encoding, which is used by most of the recently released models.\\nAs demonstrated in the example below, this method also divides a word\\nsuch as “coloring” into two parts.\\n[\"The\", \"child\", \"\\'\", \"s\", “color”, “ing”, \"book\", \".\"]\\nSubword tokenization further enhances the model’s language understanding\\nby splitting words into meaningful segments, like breaking “coloring” into\\n“color” and “ing.” This expands the model’s vocabulary and improves its\\nability to grasp the nuances of language structure and morphology.\\nUnderstanding that the “ing” part of a word indicates the present tense\\nallows us to simplify how we represent words in different tenses. We no\\nlonger need to keep separate entries for the base form of a word, like “play,”\\nand its present tense form, “playing.” By combining “play” with “ing,” we\\ncan express “playing” without needing two separate entries. This method\\nincreases the number of tokens to represent a piece of text but dramatically\\nreduces the number of tokens we need to have in the dictionary.\\nThe tokenization process involves scanning the entire text to identify\\nunique tokens, which are then indexed to create a dictionary. This\\ndictionary assigns a unique token ID to each token, enabling a standardized\\nnumerical representation of the text. When interacting with the models, this\\nconversion of text into token IDs allows the model to efficiently process\\nand understand the input, as it can quickly reference the dictionary to\\ndecode the meaning of each token. We will see an example of this process\\nlater in the book.'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 5, 'page_label': '6'}, page_content='Once we have our tokens, we can process the inner workings of\\ntransformers: embeddings.\\nEmbeddings\\nThe next step after tokenization is to turn these tokens into something the\\ncomputer can understand and work with—this is where embeddings come\\ninto play. Embeddings are a way to translate the tokens, which are words or\\npieces of words, into a language of numbers that the computer can grasp.\\nThey help the model understand relationships and context. They allow the\\nmodel to see connections between words and use these connections to\\nunderstand text better, mainly through the attention process, as we will see.\\nAn embedding gives each token a unique numerical ID that captures its\\nmeaning. This numerical form helps the computer see how similar two\\ntokens are, like knowing that “happy” and “joyful” are close in meaning,\\neven though they are different words.\\nThis step is essential because it helps the model make sense of language in\\na numerical way, bridging the gap between human language and machine\\nprocessing.\\nInitially, every token is assigned a random set of numbers as its embedding.\\nAs the model is trained—meaning as it reads and learns from lots of text—\\nit adjusts these numbers. The goal is to tweak them so that tokens with\\nsimilar meanings end up with similar sets of numbers. This adjustment is\\ndone automatically by the model as it learns from different contexts in\\nwhich the tokens appear.\\nWhile the concept of numerical sets, or vectors, might sound complex, they\\nare just a way for the model to store and process information about tokens\\nefficiently. We use vectors because they are a straightforward method for\\nthe model to keep track of how tokens are related to each other. They are\\nbasically just large lists of numbers.\\nIn Chapter 2, we’ll explore more about how these embeddings are created\\nand used in the transformer architecture.'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 6, 'page_label': '7'}, page_content='Training/Fine-Tuning\\nLLMs are trained on a large corpus of text with the objective of correctly\\npredicting the next token of a sequence. As we learned in the previous\\nlanguage modeling subsection, the goal is to adjust the model’s parameters\\nto maximize the probability of a correct prediction based on the observed\\ndata. Typically, a model is trained on a huge general-purpose dataset of\\ntexts from the Internet, such as The Pile or CommonCrawl. Sometimes,\\nmore specific datasets, such as the StackOverflow Posts dataset, are also an\\nexample of acquiring domain-specific knowledge. This phase is also known\\nas the pre-training stage, indicating that the model is trained to learn\\nlanguage comprehension and is prepared for further tuning.\\nThe training process adjusts the model’s weights to increase the\\nlikelihood of predicting the next token in a sequence. This adjustment\\nis based on the training data, guiding the model towards accurate token\\npredictions.\\nAfter pre-training, the model typically undergoes fine-tuning for a specific\\ntask. This stage requires further training on a smaller dataset for a task (e.g.,\\ntext translation) or a specialized domain (e.g., biomedical, finance, etc.).\\nFine-tuning allows the model to adjust its previous knowledge of the\\nspecific task or domain, enhancing its performance.\\nThe fine-tuning process can be intricate, particularly for advanced models\\nsuch as GPT-4. These models employ advanced techniques and leverage\\nlarge volumes of data to achieve their performance levels.\\nPrediction\\nThe model can generate text after the training or fine-tuning phase by\\npredicting subsequent tokens in a sequence. This is achieved by inputting\\nthe sequence into the model, producing a probability distribution over the\\npotential next tokens, essentially assigning a score to every word in the\\nvocabulary. The next token is selected according to its score. The generation\\nprocess will be repeated in a loop to predict one word at a time, so'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 7, 'page_label': '8'}, page_content='generating sequences of any length is possible. However, keeping the\\nmodel’s effective context size in mind is essential.\\nContext Size\\nThe context size, or context window, is a crucial aspect of LLMs. It refers\\nto the maximum number of tokens the model can process in a single\\nrequest. Context size influences the length of text the model can handle at\\nany one time, directly affecting the model’s performance and the outcomes\\nit produces.\\nDifferent LLMs are designed with varying context sizes. For example,\\nOpenAI’s “gpt-3.5-turbo-16k” model has a context window capable of\\nhandling 16,000 tokens. There is an inherent limit to the number of tokens a\\nmodel can generate. Smaller models may have a capacity of up to 1,000\\ntokens, while larger ones like GPT-4 can manage up to 32,000 tokens as of\\nthe time we wrote this book.\\nScaling Laws\\nScaling laws describe the relationship between a language model’s\\nperformance and various factors, including the number of parameters, the\\ntraining dataset size, the compute budget, and the network architecture.\\nThese laws, elaborated in the Chinchilla paper, provide useful insights on\\nresource allocation for successful model training. They are also a source of\\nmany memes from the “scaling is all you need” side of the community in\\nAI.\\nThe following elements determine a language model’s performance:\\n1. The number of parameters (N) denotes the model’s ability to\\nlearn from data. A greater number of parameters enables the\\ndetection of more complicated patterns in data.\\n2. The size of the Training Dataset (D) and the number of tokens,\\nranging from small text chunks to single characters, are counted.'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 8, 'page_label': '9'}, page_content='3. FLOPs (Floating Point Operations Per Second) estimate the\\ncomputational resources used during training.\\nIn their research, the authors trained the Chinchilla model, which comprises\\n70 billion parameters, on a dataset of 1.4 trillion tokens. This approach\\naligns with the scaling law proposed in the paper: for a model with X\\nparameters, the optimal training involves approximately X * 20 tokens.\\nFor example, a model with 100 billion parameters would ideally be trained\\non about 2 trillion tokens.\\nWith this approach, despite its smaller size compared to other LLMs, the\\nChinchilla model outperformed them all. It improved language modeling\\nand task-specific performance using less memory and computational power.\\nFind the paper “Training Compute-Optimal Large Language Models.” at\\ntowardsai.net/book.\\nEmergent Abilities in LLMs\\nEmergent abilities in LLMs describe the phenomena in which new skills\\nemerge unexpectedly as model size grows. These abilities, including\\narithmetic, answering questions, summarizing material, and others, are not\\nexplicitly taught to the model throughout its training. Instead, they emerge\\nspontaneously when the model’s scaling increases, hence the word\\n“emergent.”\\nLLMs are probabilistic models that learn natural language patterns. When\\nthese models are ramped up, their pattern recognition capacity improves\\nquantitatively while also changing qualitatively.\\nTraditionally, models required task-specific fine-tuning and architectural\\nadjustments to execute specific tasks. However, scaled-up models can\\nperform these jobs without architectural changes or specialized tuning.\\nThey accomplish this by interpreting tasks using natural language\\nprocessing. LLMs’ ability to accomplish various functions without explicit\\nfine-tuning is a significant milestone.'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 9, 'page_label': '10'}, page_content='What’s more remarkable is how these abilities show themselves. LLMs\\nswiftly and unpredictably progress from near-zero to sometimes state-of-\\nthe-art performance as their size grows. This phenomenon indicates that\\nthese abilities arise from the model’s scale rather than being clearly\\nprogrammed into the model.\\nThis growth in model size and the expansion of training datasets,\\naccompanied by substantial increases in computational costs, paved the way\\nfor the emergence of today’s Large Language Models. Examples of such\\nmodels include Cohere Command, GPT-4, and LLaMA, each representing\\nsignificant milestones in the evolution of language modeling.\\nPrompts\\nThe text (or images, numbers, tables…) we provide to LLMs as instructions\\nis commonly called prompts. Prompts are instructions given to AI systems\\nlike OpenAI’s GPT-3 and GPT-4, providing context to generate human-like\\ntext—the more detailed the prompt, the better the model’s output.\\nConcise, descriptive, and short (depending on the task) prompts generally\\nlead to more effective results, allowing for the LLM’s creativity while\\nguiding it toward the desired output. Using specific words or phrases can\\nhelp focus the model on generating relevant content. Creating effective\\nprompts requires a clear purpose, keeping things simple, strategically using\\nkeywords, and assuring actionability. Testing prompts before final use is\\ncritical to ensure the output is relevant and error-free. Here are some\\nprompting tips:\\n1. Use Precise Language: Precision in your prompt can\\nsignificantly improve the accuracy of the output.\\nLess Precise: “Write about dog food.”\\nMore Precise: “Write a 500-word informative article about\\nthe dietary needs of adult Golden Retrievers.”\\n1. Provide Sufficient Context: Context helps the model\\nunderstand the expected output:\\nLess Contextual: “Write a story.”'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 10, 'page_label': '11'}, page_content='More Contextual: “Write a short story set in Victorian\\nEngland featuring a young detective solving his first major\\ncase.”\\n1. Test Variations: Experiment with different prompt styles to find\\nthe most effective approach:\\nInitial: “Write a blog post about the benefits of yoga.”\\nVariation 1: “Compose a 1000-word blog post detailing the\\nphysical and mental benefits of regular yoga practice.”\\nVariation 2: “Create an engaging blog post that highlights\\nthe top 10 benefits of incorporating yoga into a daily\\nroutine.”\\n1. Review Outputs: Always double-check automated outputs for\\naccuracy and relevance before publishing.\\nBefore Review: “Yoga is a great way to improve your\\nflexibility and strength. It can also help reduce stress and\\nimprove mental clarity. However, it’s important to\\nremember that all yoga poses are suitable for everyone.”\\nAfter Review (corrected): “Yoga is a great way to improve\\nyour flexibility and strength. It can also help reduce stress\\nand improve mental clarity. However, it’s important to\\nremember that not all yoga poses are suitable for everyone.\\nAlways consult with a healthcare professional before\\nstarting any new exercise regimen.”\\nHallucinations and Biases in LLMs\\nHallucinations in AI systems refer to instances where these systems\\nproduce outputs, such as text or visuals, inconsistent with facts or the\\navailable inputs. One example would be if ChatGPT provides a compelling\\nbut factually wrong response to a question. These hallucinations show a\\nmismatch between the AI’s output and real-world knowledge or context.\\nIn LLMs, hallucinations occur when the model creates outputs that do\\nnot correspond to real-world facts or context. This can lead to the\\nspread of disinformation, especially in crucial industries like'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 11, 'page_label': '12'}, page_content='healthcare and education, where information accuracy is critical. Bias\\nin LLMs can also result in outcomes that favor particular perspectives\\nover others, possibly reinforcing harmful stereotypes and\\ndiscrimination.\\nAn example of a hallucination could be if a user asks, “Who won the World\\nSeries in 2025?” and the LLM responds with a specific winner. As of the\\ncurrent date (Jan 2024), the event has yet to occur, making any response\\nspeculative and incorrect.\\nAdditionally, Bias in AI and LLMs is another critical issue. It refers to these\\nmodels’ inclination to favor specific outputs or decisions based on their\\ntraining data. If the training data primarily originates from a particular\\nregion, the model may be biased toward that region’s language, culture, or\\nviewpoints. In cases where the training data encompasses biases, like\\ngender or race, the resulting outputs from the AI system could be biased or\\ndiscriminatory.\\nFor example, if a user asks an LLM, “Who is a nurse?” and it responds, “She is\\na healthcare professional who cares for\\npatients in a hospital,” this demonstrates a gender bias. The paradigm\\ninherently associates nursing with women, which needs to adequately\\nreflect the reality that both men and women can be nurses.\\nMitigating hallucinations and bias in AI systems involves refining model\\ntraining, using verification techniques, and ensuring the training data is\\ndiverse and representative. Finding a balance between maximizing the\\nmodel’s potential and avoiding these issues remains challenging.\\nAmazingly, these “hallucinations” might be advantageous in creative fields\\nsuch as fiction writing, allowing for the creation of new and novel content.\\nThe ultimate goal is to create powerful, efficient but also trustworthy, fair,\\nand reliable LLMs. We can maximize the promise of LLMs while\\nminimizing their hazards, ensuring that the advantages of this technology\\nare available to all.'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 12, 'page_label': '13'}, page_content='Translation with LLMs (GPT-3.5 API)\\nNow, we can combine all we have learned to demonstrate how to interact\\nwith OpenAI’s proprietary LLM through their API, instructing the model to\\nperform translation. To generate text using LLMs like those provided by\\nOpenAI, you first need an API key for your Python environment. Here’s a\\nstep-by-step guide to generating this key:\\n1. Create and log into your OpenAI account.\\n2. After logging in, select ‘Personal’ from the top-right menu and\\nclick “View API keys.”\\n3. You’ll find the “Create new secret key” button on the API keys\\npage. Click on it to generate a new secret key. Remember to save\\nthis key securely, as it will be used later.\\nAfter generating your API key, you can securely store it in a .env file using\\nthe following format:\\nOPENAI_API_KEY=\"<YOUR-OPENAI-API-KEY>\"\\nEvery time you initiate a Python script including the following lines, your\\nAPI key will be automatically loaded into an environment variable named\\nOPENAI_API_KEY. The openai library subsequently uses this variable for text\\ngeneration tasks. The .env file must be in the same directory as the Python\\nscript.\\nfrom dotenv import load_dotenv\\nload_dotenv()\\nNow, the model is ready for interaction! Here’s an example of using the\\nmodel for a language translation from English to French. The code below\\nsends the prompt as a message with a user role, using the OpenAI Python\\npackage to send and retrieve requests from the API. There is no need for\\nconcern if you do not understand all the details, as we will use the OpenAI\\nAPI more thoroughly in Chapter 5. It would be best if you focused on the'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 13, 'page_label': '14'}, page_content='messages argument for now, which receives the prompt that directs the\\nmodel to execute the translation task.\\nfrom dotenv import load_dotenv\\nload_dotenv()\\nimport os\\nimport openai\\n# English text to translate\\nenglish_text = \"Hello, how are you?\"\\nresponse = openai.ChatCompletion.create(\\n  model=\"gpt-3.5-turbo\",\\n  messages=[\\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n    {\"role\": \"user\", \"content\": f\\'\\'\\'Translate the following English text\\nto French: \"{english_text}\"\\'\\'\\'}\\n  ],\\n)\\nprint(response[\\'choices\\'][0][\\'message\\'][\\'content\\'])\\nBonjour, comment ça va?\\n💡 You can safely store sensitive information, such as API keys, in a\\nseparate file with dotenv and avoid accidentally exposing it in your code.\\nThis is especially important when working with open-source projects or\\nsharing your code with others, as it ensures the security of sensitive\\ninformation.\\nControl LLMs Output by Providing Examples\\nFew-shot learning, which is one of the emergent abilities of LLMs, means\\nproviding the model with a small number of examples before making\\npredictions. These examples serve a dual purpose: they “teach” the model in\\nits reasoning process and act as “filters,” aiding the model in identifying\\nrelevant patterns within its dataset. Few-shot learning allows for the\\nadaptation of the model to new tasks. While LLMs like GPT-3 show\\nproficiency in language modeling tasks such as machine translation, their\\nperformance can vary on tasks that require more complex reasoning.'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 14, 'page_label': '15'}, page_content='In few-shot learning, the examples presented to the model help discover\\nrelevant patterns in the dataset. The datasets are effectively encoded into the\\nmodel’s weights during the training, so the model looks for patterns that\\nsignificantly connect with the provided samples and uses them to generate\\nits output. As a result, the model’s precision improves by adding more\\nexamples, allowing for a more targeted and relevant response.\\nHere is an example of few-shot learning, where we provide examples\\nthrough different message types on how to describe movies with emojis to\\nthe model. (We will cover the different message types later in the book.) For\\ninstance, the movie “Titanic” might be presented using emojis for a cruise\\nship, waves, a heart, etc., or how to represent “The Matrix” movie. The\\nmodel picks up on these patterns and manages to accurately describe the\\nmovie “Toy Story” using emojis of toys.\\nfrom dotenv import load_dotenv\\nload_dotenv()\\nimport os\\nimport openai\\n# Prompt for summarization\\nprompt = \"\"\"\\nDescribe the following movie using emojis.\\n{movie}: \"\"\"\\nexamples = [\\n    { \"input\": \"Titanic\", \"output\": \" 🛳🌊❤🧊🎶🔥🚢💔👫💑 \" },\\n    { \"input\": \"The Matrix\", \"output\": \" 🕶💊💥👾🔮🌃\\x00🔁🔓💪 \" }\\n]\\nmovie = \"Toy Story\"\\nresponse = openai.ChatCompletion.create(\\n  model=\"gpt-3.5-turbo\",\\n  messages=[\\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n        {\"role\": \"user\", \"content\": prompt.format(movie=examples[0]\\n[\"input\"])},\\n        {\"role\": \"assistant\", \"content\": examples[0][\"output\"]},\\n        {\"role\": \"user\", \"content\": prompt.format(movie=examples[1]\\n[\"input\"])},\\n        {\"role\": \"assistant\", \"content\": examples[1][\"output\"]},\\n        {\"role\": \"user\", \"content\": prompt.format(movie=movie)},'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 15, 'page_label': '16'}, page_content=\"]\\n)\\nprint(response['choices'][0]['message']['content'])\\n🧸🤠👦🧒🎢🌈🌟👫🚁👽🐶🚀\\nIt’s fascinating how the model, with just two examples, can identify a\\ncomplex pattern, such as associating a film title with a sequence of emojis.\\nThis ability is achievable only with a model that possesses an in-depth\\nunderstanding of the film’s story and the meaning of the emojis, allowing it\\nto merge the two and respond to inquiries based on its own interpretation. \\nFrom Language Models to Large\\nLanguage Models\\nThe evolution of language models has seen a paradigm shift from pre-\\ntrained language models\\n(LMs) to the creation of Large Language Models (LLMs). LMs, such as\\nELMo and BERT, first captured context-aware word representations\\nthrough pre-training and fine-tuning for specific tasks. However, the\\nintroduction of LLMs, as demonstrated by GPT-3 and PaLM, proved that\\nscaling model size and data can unlock emergent skills that outperform their\\nsmaller counterparts. Through in-context learning, these LLMs can handle\\nmore complex tasks.\\nEmergent Abilities in LLMs\\nAs we discussed, an ability is considered emergent when larger models\\nexhibit it, but it’s absent in smaller models—a key factor contributing to the\\nsuccess of Large Language Models. Emergent abilities in Large Language\\nModels (LLMs) are empirical phenomena that occur when the size of\\nlanguage models exceeds specific thresholds. As we increase the models’\\nsize, emergent abilities become more evident, influenced by aspects like the\\ncomputational power used in training and the model’s parameters.\"),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 16, 'page_label': '17'}, page_content='What Are Emergent Abilities\\nThis phenomenon indicates that the models are learning and generalizing\\nbeyond their pre-training in ways that were not explicitly programmed or\\nanticipated. A distinct pattern emerges when these abilities are depicted on a\\nscaling curve. Initially, the model’s performance appears almost random,\\nbut it significantly improves once a certain scale threshold is reached. This\\nphenomenon is known as a phase transition, representing a dramatic\\nbehavior change that would not have been apparent from examining\\nsmaller-scale systems.\\nScaling language models have predominantly focused on increasing the\\namount of computation, expanding the model parameters, and enlarging the\\ntraining dataset size. New abilities can sometimes emerge with reduced\\ntraining computation or fewer model parameters, especially when models\\nare trained on higher-quality data. Additionally, the appearance of emergent\\nabilities is influenced by factors such as the volume and quality of the data\\nand the quantity of the model’s parameters. Emergent abilities in Large\\nLanguage Models surface as the models are scaled up and are not\\npredictable by merely extending the trends observed in smaller models.\\nEvaluation Benchmarks for Emergent Abilities\\nSeveral benchmarks are used to evaluate the emergent abilities of language\\nmodels, such as BIG-Bench, TruthfulQA, the Massive Multi-task Language\\nUnderstanding (MMLU) benchmark, and the Word in Context (WiC)\\nbenchmark. Key benchmarks include:\\n1. BIG-Bench suite comprises over 200 benchmarks testing a wide\\narray of tasks, such as arithmetic operations (example: “Q: What\\nis 132 plus 762? A: 894), transliteration from the International\\nPhonetic Alphabet (IPA), and word unscrambling. These tasks\\nassess a model’s capacity to perform calculations, manipulate\\nand use rare words, and work with alphabets. (example:\\n“English: The 1931 Malay census was an alarm bell. IPA: ðə\\n1931 ˈ me ɪ le ɪ  ˈ s ɛ nsəs w ɑ z ən ə ˈ l ɑ rm b ɛ l.”) The performance of\\nmodels like GPT-3 and LaMDA on these tasks usually starts near'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 17, 'page_label': '18'}, page_content='zero but shows a significant increase above random at a certain\\nscale, indicative of emergent abilities. More details on these\\nbenchmarks can be found in the Github repository.\\n2. TruthfulQA benchmark evaluates a model’s ability to provide\\ntruthful responses. It includes two tasks: generation, where the\\nmodel answers a question in one or two sentences, and multiple-\\nchoice, where the model selects the correct answer from four\\noptions or True/False statements. As the Gopher model is scaled\\nto its largest size, its performance improves significantly,\\nexceeding random outcomes by over 20%, which signifies the\\nemergence of this ability.\\n3. Massive Multi-task Language Understanding (MMLU)\\nassesses a model’s world knowledge and problem-solving skills\\nacross 57 diverse tasks, including elementary mathematics, US\\nhistory, and computer science. While GPTs, Gopher, and\\nChinchilla models of a certain scale do not outperform random\\nguessing on average across all topics, a larger size model shows\\nimproved performance, suggesting the emergence of this ability.\\n4. The Word in Context (WiC) benchmark focuses on semantic\\nunderstanding and involves a binary classification task for\\ncontext-sensitive word embeddings. It requires determining if\\ntarget words (verbs or nouns) in two contexts share the same\\nmeaning. Models like Chinchilla initially fail to surpass random\\nperformance in one-shot tasks, even at large scales. However,\\nwhen models like PaLM are scaled to a much larger size, above-\\nrandom performance emerges, indicating the emergence of this\\nability at a larger scale.\\nFactors Leading To Emergent Abilities\\n \\n• Multi-step reasoning involves instructing a model to perform a\\nseries of intermediate steps before providing the final result. This\\napproach, known as chain-of-thought prompting, becomes more\\neffective than standard prompting only when applied to sufficiently\\nlarge models.'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 18, 'page_label': '19'}, page_content='• Another strategy is fine-tuning a model on various tasks presented\\nas Instruction Following. This method shows improved performance\\nonly with models of a certain size, underlining the significance of\\nscale in achieving advanced capabilities.\\nRisks With Emergent Abilities\\nAs language models are scaled up, emergent risks also become a concern.\\nThese include societal challenges related to accuracy, bias, and toxicity.\\nAdopting strategies that encourage models to be “helpful, harmless, and\\nhonest” can mitigate these risks.\\nFor instance, the WinoGender benchmark, which assesses gender bias in\\noccupational contexts, has shown that while scaling can enhance model\\nperformance, it may also amplify biases, especially in ambiguous situations.\\nLarger models tend to memorize training data more, but methods like\\ndeduplication can reduce this risk.\\nOther risks involve potential vulnerabilities or harmful content synthesis\\nthat might be more prevalent in future language models or remain\\nuncharacterized in current models.\\nA Shift Towards General-Purpose Models\\nThe emergence of new abilities has shifted the NLP community’s\\nperspective and utilization of these models. While NLP traditionally\\nfocused on task-specific models, the scaling of models has spurred research\\non “general-purpose” models capable of handling a wide range of tasks not\\nexplicitly included in their training.\\nThis shift is evident in instances where scaled, few-shot prompted general-\\npurpose models have outperformed task-specific models that were fine-\\ntuned. Examples include GPT-3 setting new benchmarks in TriviaQA and\\nPiQA, PaLM excelling in arithmetic reasoning, and the multimodal\\nFlamingo model achieving top performance in visual question answering.\\nFurthermore, the ability of general-purpose models to execute tasks with\\nminimal examples has expanded their applications beyond traditional NLP'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 19, 'page_label': '20'}, page_content='research. These include translating natural language instructions for robotic\\nexecution, user interaction, and multi-modal reasoning.\\nExpanding the Context Window\\nThe Importance of Context Length\\nContext window in language models represents the number of input tokens\\nthe model can process simultaneously. In models like GPT-4, it currently\\nstands at approximately 32K or roughly 50 pages of text. However, recent\\nadvancements have extended this to an impressive 100K tokens or about\\n156 pages, as seen in Claude by Anthropic.\\nContext length primarily enables the model to process and comprehend\\nlarger datasets simultaneously, offering a deeper understanding of the\\ncontext. This feature is particularly beneficial when inputting a substantial\\namount of specific data into a language model and posing questions related\\nto this data. For example, when analyzing a lengthy document about a\\nparticular company or issue, a larger context window allows the language\\nmodel to review and remember more of this unique information, resulting\\nin more accurate and tailored responses.\\nLimitations of the Original Transformer\\nArchitecture\\nDespite its strengths, the original transformer architecture faces challenges\\nin handling extensive context lengths. Specifically, the attention layer\\noperations in the transformer have quadratic time and space complexity\\n(represented with ) in relation to the number of input tokens, . As the\\ncontext length expands, the computational resources required for training\\nand inference increase substantially.\\nTo better understand this, let’s examine the computational complexity of the\\ntransformer architecture. The complexity of the attention layer in the\\ntransformer model is , where is the context length (number of input tokens)\\nand is the embedding size.'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 20, 'page_label': '21'}, page_content='This complexity stems from two primary operations in the attention layer:\\nlinear projections to create Query, Key, and Value matrices (complexity ~ )\\nand the multiplication of these matrices (complexity ~ ). As the context\\nlength or embedding size increases, the computational complexity also\\ngrows quadratically, presenting a challenge for processing larger context\\nlengths.\\nOptimization Techniques to Expand the Context\\nWindow\\nDespite the computational challenges associated with the original\\ntransformer architecture, researchers have developed a range of\\noptimization techniques to enhance the transformer’s efficiency and\\nincrease its context length capacity to 100K tokens:\\n1. ALiBi Positional Encoding: The original transformer used\\nPositional Sinusoidal Encoding, which has trouble inferring\\nlarger context lengths. On the other hand, ALiBi (Attention with\\nLinear Biases) is a more scalable solution. This positional\\nencoding technique allows the model to be trained in smaller\\ncontexts and then fine-tuned in bigger contexts, making it more\\nadaptive to different context sizes.\\n2. Sparse Attention: Sparse Attention addresses the computational\\nchallenge by focusing attention scores on a subset of tokens.\\nThis method significantly decreases the computing complexity to\\na linear scale with respect to the number of tokens n, resulting in\\na significant reduction in overall computational demand.\\n3. FlashAttention: FlashAttention restructures the attention layer\\ncalculation for GPU efficiency. It divides input matrices into\\nblocks and then processes attention output with reference to\\nthese blocks, optimizing GPU memory utilization and increasing\\nprocessing efficiency.\\n4. Multi-Query Attention (MQA): MQA reduces memory\\nconsumption in the key/value decoder cache by aggregating\\nweights across all attention heads during linear projection of the'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 21, 'page_label': '22'}, page_content='Key and Value matrices. This consolidation results in more\\neffective memory utilization.\\nFlashAttention-2\\nFlashAttention-2 emerges as an advancement over the original\\nFlashAttention, focusing on optimizing the speed and memory efficiency of\\nthe attention layer in transformer models. This upgraded version is\\nredeveloped from the ground up utilizing Nvidia’s new primitives. It\\nperforms approximately 2x faster than its predecessor, achieving up to 230\\nTFLOPs on A100 GPUs.\\nFlashAttention-2 improves on the original FlashAttention in various ways.\\n• Changing the algorithm to spend more time on matmul FLOPs\\nminimizes the quantity of non-matmul FLOPs, which are 16x more\\nexpensive than matmul FLOPs.\\n• It optimizes parallelism across batch size, headcount, and sequence\\nlength dimensions, leading to significant acceleration, particularly for\\nlong sequences.\\n• It enhances task partitioning within each thread block to reduce\\nsynchronization and communication between warps, resulting in\\nfewer shared memory reads/writes.\\n• It adds features such as support for attention head dimensions up to\\n256 and multi-query attention (MQA), further expanding the context\\nwindow.\\nWith these enhancements, FlashAttention-2 is a successful step toward\\ncontext window expansion (while still retaining the underlying restrictions\\nof the original transformer architecture).'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 22, 'page_label': '23'}, page_content='LongNet: A Leap Towards Billion-Token Context\\nWindow\\nLongNet represents a transformative advancement in the field of\\ntransformer optimization, as detailed in the paper “LONGNET: Scaling\\nTransformers to 1,000,000,000 Tokens”. This innovative approach is set to\\nextend the context window of language models to an unprecedented 1\\nbillion tokens, significantly enhancing their ability to process and analyze\\nlarge volumes of data.\\nThe primary advancement in LongNet is the implementation of “dilated\\nattention.” This innovative attention mechanism allows for an exponential\\nincrease in the attention field as the gap between tokens widens, inversely\\nreducing attention calculations as the distance between tokens increases.\\n(since every token will attend to a smaller number of tokens). This design\\napproach balances the limited attention resources and the need to access\\nevery token in the sequence.\\nLongNet’s dilated attention mechanism has a linear computational\\ncomplexity, a major improvement over the normal transformer’s quadratic\\ndifficulty.\\nA Timeline of the Most Popular LLMs\\nHere’s the timeline of some of the most popular LLMs in the last five years.\\n• [2018]GPT-1\\n Introduced by OpenAI, GPT-1 laid the foundation for the GPT series\\nwith its generative, decoder-only transformer architecture. It\\npioneered the combination of unsupervised pretraining and supervised\\nfine-tuning for natural language text prediction.\\n• [2019]GPT-2\\n Building on GPT-1’s architecture, GPT-2 expanded the model size to\\n1.5 billion parameters, demonstrating the model’s versatility across a\\nrange of tasks using a unified format for input, output, and task\\ninformation.'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 23, 'page_label': '24'}, page_content='• [2020]GPT-3\\n Released in 2020, GPT-3 marked a substantial leap with 175 billion\\nparameters, introducing in-context learning (ICL). This model\\nshowcased exceptional performance in various NLP tasks, including\\nreasoning and domain adaptation, highlighting the potential of scaling\\nup model size.\\n• [2021]Codex\\n OpenAI introduced Codex in July 2021. It is a GPT-3 variant fine-\\ntuned on a corpus of GitHub code and exhibited advanced\\nprogramming and mathematical problem-solving capabilities,\\ndemonstrating the potential of specialized training.\\n• [2021]LaMDA\\n Researchers from DeepMind introduced LaMDA (Language Models\\nfor Dialog Applications). LaMDA focused on dialog applications,\\nboasting 137 billion parameters. It aimed to enhance dialog\\ngeneration and conversational AI.\\n• [2021]Gopher\\n In 2021, DeepMind’s Gopher, with 280 billion parameters,\\napproached human-level performance on the MMLU benchmark but\\nfaced challenges like biases and misinformation.\\n• [2022]InstructGPT\\n In 2022, InstructGPT, an enhancement to GPT-3, utilized\\nreinforcement learning from human feedback to improve instruction-\\nfollowing and content safety, aligning better with human preferences\\n• [2022]Chinchilla\\nDeepMind’s Chinchilla introduced in 2022, with 70 billion\\nparameters, optimized compute resource usage based on scaling laws,\\nachieving significant accuracy improvements on benchmarks.\\n• [2022]PaLM\\n Pathways Language Model (PaLM) was introduced by Google\\nResearch in 2022. Google’s PaLM, with an astounding 540 billion\\nparameters, demonstrated exceptional few-shot performance,\\nbenefiting from Google’s Pathways system for distributed\\ncomputation.\\n• [2022]ChatGPT'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 24, 'page_label': '25'}, page_content='In November 2022, OpenAI’s ChatGPT, based on GPT-3.5 and GPT-\\n4, was tailored for conversational AI and showed proficiency in\\nhuman-like communication and reasoning.\\n• [2023]LLaMA\\n Meta AI developed LLaMA (Large Language Model Meta AI) in\\nFebruary 2023. It introduced a family of massive language models\\nwith parameters ranging from 7 billion to 65 billion. The publication\\nof LLaMA broke the tradition of limited access by making its model\\nweights available to the scientific community under a noncommercial\\nlicense. Subsequent innovations, such as LLaMA 2 and other chat\\nformats, stressed accessibility even further, this time with a\\ncommercial license.\\n• [2023]GPT-4\\n In March 2023, GPT-4 expanded its capabilities to multimodal\\ninputs, outperforming its predecessors in various tasks and\\nrepresenting another significant step in LLM development.\\n• [2024]Gemini 1.5\\n Gemini 1.5 (from Google) features a significant upgrade compared to\\nthe previous iteration of the model with a new Mixture-of-Experts\\narchitecture and multimodal model capability, Gemini 1.5 Pro, which\\nsupports advanced long-context understanding and a context window\\nof up to 1 million tokens. The context window size is larger than any\\nother model available today. The model is accessible through\\nGoogle’s proprietary API.\\n• [2024]Gemma\\n Google has also released the Gemma model in two versions: 2 billion\\nand 7 billion parameters. These models were developed during the\\ntraining phase that produced the Gemini model and are now publicly\\naccessible. Users can access these models in both pre-trained and\\ninstruction-tuned formats.\\n• [2024]Claude 3 Opus\\n The newest model from Anthropic, the Claude 3 Opus, is available\\nvia their proprietary API. It is one of the first models to achieve\\nscores comparable to or surpassing GPT-4 across different\\nbenchmarks. With a context window of 200K tokens, it is advertised\\nfor its exceptional recall capabilities, regardless of the position of the\\ninformation within the window.'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 25, 'page_label': '26'}, page_content='• [2024]Mistral\\n Following their publication detailing the Mixture of Experts\\narchitecture, they have now made the 8x22 billion base model\\navailable to the public. This model is the best open-source option\\ncurrently accessible for use. Despite this, it still does not outperform\\nthe performance of closed-source models like GPT-4 or Claude.\\n• [2024]Infinite Attention\\n Google’s recent paper, speculated to be the base of the Gemini 1.5\\nPro model, explores techniques that could indefinitely expand the\\nmodel’s context window size. Speculation surfaced because the paper\\nreleased alongside the Gemini model mentioned that the model could\\nperform exceptionally well with up to 10 million tokens. However, a\\nmodel with these specifications has yet to be released. This approach\\nis described as a plug-and-play solution that can significantly enhance\\nany model’s few-shot learning performance without context size\\nconstraints.\\n If you want to dive deeper into these models, we suggest reading the\\npaper “A Survey of Large Language Models”.\\nHistory of NLP/LLMs\\nThis is a journey through the growth of language modeling models, from\\nearly statistical models to the birth of the first Large Language Models\\n(LLMs). Rather than an in-depth technical study, this chapter presents a\\nstory-like exploration of model building. Don’t worry if certain model\\nspecifics appear complicated.\\nThe Evolution of Language Modeling\\nThe evolution of natural language processing (NLP) models is a story of\\nconstant invention and improvement. The Bag of Words model, a simple\\napproach for counting word occurrences in documents, began in 1954.\\nThen, in 1972, TF-IDF appeared, improving on this strategy by altering\\nword counts based on rarity or frequency. The introduction of Word2Vec in\\n2013 marked a significant breakthrough. This model used word embeddings'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 26, 'page_label': '27'}, page_content='to capture subtle semantic links between words that previous models could\\nnot.\\nFollowing that, Recurrent Neural Networks (RNNs) were introduced.\\nRNNs were adept at learning patterns in sequences, allowing them to\\nhandle documents of varied lengths effectively.\\nThe launch of the transformer architecture in 2017 signified a paradigm\\nchange in the area. During output creation, the model’s attention\\nmechanism allowed it to focus on the most relevant elements of the input\\nselectively. This breakthrough paved the way for BERT in 2018. BERT\\nused a bidirectional transformer, significantly increasing performance in\\nvarious traditional NLP workloads.\\nThe years that followed saw a rise in model developments. Each new\\nmodel, such as RoBERTa, XLM, ALBERT, and ELECTRA, introduced\\nadditional enhancements and optimizations, pushing the bounds of what\\nwas feasible in NLP.\\nModel’s Timeline\\n \\n• [1954]Bag of Words (BOW)\\n The Bag of Words model was a basic approach that tallied word\\noccurrences in manuscripts. Despite its simplicity, it could not\\nconsider word order or context.\\n• [1972]TF-IDF\\n TF-IDF expanded on BOW by giving more weight to rare words and\\nless to common terms, improving the model’s ability to detect\\ndocument relevancy. Nonetheless, it made no mention of word\\ncontext.\\n• [2013]Word2Vec\\n Word embeddings are high-dimensional vectors encapsulating\\nsemantic associations, as described by Word2Vec. This was a\\nsubstantial advancement in capturing textual semantics.\\n• [2014]RNNs in Encoder-Decoder architectures'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 27, 'page_label': '28'}, page_content='RNNs were a significant advancement, capable of computing\\ndocument embeddings and adding word context. They grew to\\ninclude LSTM (1997) for long-term dependencies and Bidirectional\\nRNN (1997) for context understanding. Encoder-Decoder RNNs\\n(2014) improved on this method.\\n• [2017]Transformer\\n The transformer, with its attention mechanisms, greatly improved\\nembedding computation and alignment between input and output,\\nrevolutionizing NLP tasks.\\n• [2018]BERT\\n BERT, a bidirectional transformer, achieved impressive NLP results\\nusing global attention and combined training objectives.\\n• [2018]GPT\\n The transformer architecture was used to create the first\\nautoregressive model, GPT. It then evolved into GPT-2 [2019], a\\nlarger and more optimized version of GPT pre-trained on WebText,\\nand GPT-3 [2020], a larger and more optimized version of GPT-2 pre-\\ntrained on Common Crawl.\\n• [2019]CTRL\\n CTRL, similar to GPT, introduced control codes enabling conditional\\ntext generation. This feature enhanced control over the content and\\nstyle of the generated text.\\n• [2019]Transformer-XL\\n Transformer-XL innovated by reusing previously computed hidden\\nstates, allowing the model to maintain a longer contextual memory.\\nThis enhancement significantly improved the model’s ability to\\nhandle extended text sequences.\\n• [2019]ALBERT\\n ALBERT offered a more efficient version of BERT by implementing\\nSentence Order Prediction instead of Next Sentence Prediction and\\nemploying parameter-reduction techniques. These changes resulted in\\nlower memory usage and expedited training.\\n• [2019]RoBERTa\\n RoBERTa improved upon BERT by introducing dynamic Masked\\nLanguage Modeling, omitting the Next Sentence Prediction, using the\\nBPE tokenizer, and employing better hyperparameters for enhanced\\nperformance.'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 28, 'page_label': '29'}, page_content='• [2019]XLM\\n XLM was a multilingual transformer, pre-trained using a variety of\\nobjectives, including Causal Language Modeling, Masked Language\\nModeling, and Translation Language Modeling, catering to\\nmultilingual NLP tasks.\\n• [2019]XLNet\\n XLNet combined the strengths of Transformer-XL with a generalized\\nautoregressive pretraining approach, enabling the learning of\\nbidirectional dependencies and offering improved performance over\\ntraditional unidirectional models.\\n• [2019]PEGASUS\\nPEGASUS featured a bidirectional encoder and a left-to-right\\ndecoder, pre-trained using objectives like Masked Language\\nModeling and Gap Sentence Generation, optimizing it for\\nsummarization tasks.\\n• [2019]DistilBERT\\n DistilBERT presented a smaller, faster version of BERT, retaining\\nover 95% of its performance. This model was trained using\\ndistillation techniques to compress the pre-trained BERT model.\\n• [2019]XLM-RoBERTa\\n XLM-RoBERTa was a multilingual adaptation of RoBERTa, trained\\non a diverse multilanguage corpus, primarily using the Masked\\nLanguage Modeling objective, enhancing its multilingual capabilities.\\n• [2019]BART\\n BART, with a bidirectional encoder and a left-to-right decoder, was\\ntrained by intentionally corrupting text and then learning to\\nreconstruct the original, making it practical for a range of generation\\nand comprehension tasks.\\n• [2019]ConvBERT\\n ConvBERT innovated by replacing traditional self-attention blocks\\nwith modules incorporating convolutions, allowing for more effective\\nhandling of global and local contexts within the text.\\n• [2020]Funnel Transformer\\n Funnel Transformer innovated by progressively compressing the\\nsequence of hidden states into a shorter sequence, effectively\\nreducing computational costs while maintaining performance.'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 29, 'page_label': '30'}, page_content='• [2020]Reformer\\n Reformer offered a more efficient version of the transformer. It\\nutilized locality-sensitive hashing for attention mechanisms and axial\\nposition encoding, among other optimizations, to enhance efficiency.\\n• [2020]T5\\n T5 approached NLP tasks as a text-to-text problem. It was trained\\nusing a mixture of unsupervised and supervised tasks, making it\\nversatile for various applications.\\n• [2020]Longformer\\n Longformer adapted the transformer architecture for longer\\ndocuments. It replaced traditional attention matrices with sparse\\nversions, improving training efficiency and better handling of longer\\ntexts.\\n• [2020]ProphetNet\\n ProphetNet was trained using a Future N-gram Prediction objective,\\nincorporating a unique self-attention mechanism. This model aimed to\\nimprove sequence-to-sequence tasks like summarization and\\nquestion-answering.\\n• [2020]ELECTRA\\n ELECTRA presented a novel approach, trained with a Replaced\\nToken Detection objective. It offered improvements over BERT in\\nefficiency and performance across various NLP tasks.\\n• [2021]Switch Transformers\\n Switch Transformers introduced a sparsely-activated expert model, a\\nnew spin on the Mixture of Experts (MoE) approach. This design\\nallowed the model to manage a broader array of tasks more\\nefficiently, marking a significant step towards scaling up transformer\\nmodels.\\nRecap\\nThe advancements in natural language processing, beginning with the\\nessential Bag of Words model, led us to the advanced and highly\\nsophisticated transformer-based models we have today. Large language\\nmodels (LLMs) are powerful architectures trained on massive amounts of')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "loader=PyPDFLoader(\"Chapter-1.pdf\")\n",
        "pages=loader.load_and_split()\n",
        "pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "UDxCxIdYCGbM"
      },
      "outputs": [],
      "source": [
        "vectorstore = DocArrayInMemorySearch.from_documents(\n",
        "    pages,\n",
        "    embedding=embedding_function\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Hzv-ksoLCGbM",
        "outputId": "783cf98e-2c17-459d-ab33-6ca17468e5e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 0, 'page_label': '1'}, page_content='Chapter I: Introduction to LLMs'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 1, 'page_label': '2'}, page_content='What are Large Language Models\\nBy now, you might have heard of them. Large Language Models,\\ncommonly known as LLMs, are a sophisticated type of neural network.\\nThese models ignited many innovations in the field of natural language\\nprocessing (NLP) and are characterized by their large number of\\nparameters, often in billions, that make them proficient at processing and\\ngenerating text. They are trained on extensive textual data, enabling them to\\ngrasp various language patterns and structures. The primary goal of LLMs\\nis to interpret and create human-like text that captures the nuances of\\nnatural language, including syntax (the arrangement of words) and\\nsemantics (the meaning of words).\\nThe core training objective of LLMs focuses on predicting the next word in\\na sentence. This straightforward objective leads to the development of\\nemergent abilities. For example, they can conduct arithmetic calculations,\\nunscramble words, and have even demonstrated proficiency in professional\\nexams, such as passing the US Medical Licensing Exam. Additionally, these\\nmodels have significantly contributed to various NLP tasks, including\\nmachine translation, natural language generation, part-of-speech tagging,\\nparsing, information retrieval, and others, even without direct training or\\nfine-tuning in these specific areas.\\nThe text generation process in Large Language Models is autoregressive,\\nmeaning they generate the next tokens based on the sequence of tokens\\nalready generated. The attention mechanism is a vital component in this\\nprocess; it establishes word connections and ensures the text is coherent and\\ncontextually appropriate. It is essential to establish the fundamental\\nterminology and concepts associated with Large Language Models before\\nexploring the architecture and its building blocks (like attention\\nmechanisms) in greater depth. Let’s start with an overview of the\\narchitecture that powers these models, followed by defining a few terms,\\nsuch as language modeling and tokenization.'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 9, 'page_label': '10'}, page_content='What’s more remarkable is how these abilities show themselves. LLMs\\nswiftly and unpredictably progress from near-zero to sometimes state-of-\\nthe-art performance as their size grows. This phenomenon indicates that\\nthese abilities arise from the model’s scale rather than being clearly\\nprogrammed into the model.\\nThis growth in model size and the expansion of training datasets,\\naccompanied by substantial increases in computational costs, paved the way\\nfor the emergence of today’s Large Language Models. Examples of such\\nmodels include Cohere Command, GPT-4, and LLaMA, each representing\\nsignificant milestones in the evolution of language modeling.\\nPrompts\\nThe text (or images, numbers, tables…) we provide to LLMs as instructions\\nis commonly called prompts. Prompts are instructions given to AI systems\\nlike OpenAI’s GPT-3 and GPT-4, providing context to generate human-like\\ntext—the more detailed the prompt, the better the model’s output.\\nConcise, descriptive, and short (depending on the task) prompts generally\\nlead to more effective results, allowing for the LLM’s creativity while\\nguiding it toward the desired output. Using specific words or phrases can\\nhelp focus the model on generating relevant content. Creating effective\\nprompts requires a clear purpose, keeping things simple, strategically using\\nkeywords, and assuring actionability. Testing prompts before final use is\\ncritical to ensure the output is relevant and error-free. Here are some\\nprompting tips:\\n1. Use Precise Language: Precision in your prompt can\\nsignificantly improve the accuracy of the output.\\nLess Precise: “Write about dog food.”\\nMore Precise: “Write a 500-word informative article about\\nthe dietary needs of adult Golden Retrievers.”\\n1. Provide Sufficient Context: Context helps the model\\nunderstand the expected output:\\nLess Contextual: “Write a story.”'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250118193239', 'source': 'Chapter-1.pdf', 'total_pages': 30, 'page': 3, 'page_label': '4'}, page_content='Language Modeling\\nWith the rise of LLMs, language modeling has become an essential part of\\nnatural language processing. It means learning the probability distribution\\nof words within a language based on a large corpus. This learning process\\ntypically involves predicting the next token in a sequence using either\\nclassical statistical methods or novel deep learning techniques.\\nLarge language models are trained based on the same objective to predict\\nthe next word, punctuation mark, or other elements based on the seen\\ntokens in a text. These models become proficient by understanding the\\ndistribution of words within their training data by guessing the probability\\nof the next word based on the context. For example, the model can\\ncomplete a sentence beginning with “I live in New” with a word like\\n“York” rather than an unrelated word such as “shoe”.\\nIn practice, the models work with tokens, not complete words. This\\napproach allows for more accurate predictions and text generation by more\\neffectively capturing the complexity of human language.\\nTokenization\\nTokenization is the initial phase of interacting with LLMs. It involves\\nbreaking down the input text into smaller pieces known as tokens. Tokens\\ncan range from single characters to entire words, and the size of these\\ntokens can greatly influence the model’s performance. Some models adopt\\nsubword tokenization, breaking words into smaller segments that retain\\nmeaningful linguistic elements.\\nConsider the following sentence, “The child’s coloring book.”\\nIf tokenization splits the text after every white space character. The result\\nwill be:\\n[\"The\", \"child\\'s\", “coloring”, \"book.\"]')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "retriever=vectorstore.as_retriever()\n",
        "\n",
        "retriever.invoke(\"LLM\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "xvXa3DUjCGbM"
      },
      "outputs": [],
      "source": [
        "chain = (\n",
        "    {\n",
        "        \"context\":itemgetter(\"question\") | retriever,\n",
        "        \"question\":itemgetter(\"question\")\n",
        "    }\n",
        "    | prompt\n",
        "    | model\n",
        "    | parser\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Es1mHaw1CGbM",
        "outputId": "803133bc-1134-4dcd-d4ab-0ece34c7d33e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The PDF appears to be about Large Language Models (LLMs), specifically their introduction, applications, challenges, and potential biases. It covers topics such as language modeling, tokenization, few-shot learning, and mitigating hallucinations and bias in AI systems. The overall theme seems to be an educational or informative discussion about LLMs, likely from a textbook or academic chapter.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "chain.invoke({\"question\":\"What is the pdf about?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "lTJ0ugO_CGbM",
        "outputId": "6c7953cd-1634-471e-c3e2-ab52031f9c63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Large Language Models (LLMs) are a sophisticated type of neural network that has revolutionized the field of natural language processing (NLP). These models are characterized by their large number of parameters, often in billions, which enable them to process and generate human-like text. LLMs are trained on extensive textual data, allowing them to grasp various language patterns and structures, and their primary goal is to interpret and create text that captures the nuances of natural language, including syntax and semantics.\n",
            "\n",
            "One of the most remarkable aspects of LLMs is their ability to develop emergent abilities, such as conducting arithmetic calculations, unscrambling words, and even demonstrating proficiency in professional exams, like the US Medical Licensing Exam. This is achieved through a straightforward training objective, which focuses on predicting the next word in a sentence. The autoregressive text generation process in LLMs generates the next tokens based on the sequence of tokens already generated, with the attention mechanism playing a vital role in establishing word connections and ensuring the text is coherent and contextually appropriate.\n",
            "\n",
            "LLMs have significantly contributed to various NLP tasks, including machine translation, natural language generation, part-of-speech tagging, parsing, information retrieval, and others, even without direct training or fine-tuning in these specific areas. Their ability to swiftly and unpredictably progress from near-zero to state-of-the-art performance as their size grows is a testament to the power of these models. This phenomenon indicates that the abilities of LLMs arise from the model's scale rather than being clearly programmed into the model.\n",
            "\n",
            "The growth in model size and the expansion of training datasets, accompanied by substantial increases in computational costs, have paved the way for the emergence of today's Large Language Models. Examples of such models include Cohere Command, GPT-4, and LLaMA, each representing significant milestones in the evolution of language modeling. These models have the potential to revolutionize various industries, from healthcare and education to finance and entertainment.\n",
            "\n",
            "However, LLMs are not without their limitations. One of the significant challenges associated with these models is the issue of hallucinations, which refer to instances where the model produces outputs inconsistent with facts or available inputs. This can lead to the spread of disinformation, especially in crucial industries like healthcare and finance. Therefore, it is essential to review automated outputs for accuracy and relevance before publishing.\n",
            "\n",
            "To effectively utilize LLMs, it is crucial to provide precise and contextual prompts that guide the model towards the desired output. Concise, descriptive, and short prompts generally lead to more effective results, allowing for the model's creativity while focusing it on generating relevant content. Testing variations of prompts and reviewing outputs are also critical to ensure the accuracy and relevance of the generated text.\n",
            "\n",
            "In conclusion, Large Language Models are a powerful tool that has the potential to revolutionize the field of natural language processing. Their ability to develop emergent abilities, generate human-like text, and contribute to various NLP tasks makes them an essential component of modern AI systems. However, it is crucial to address the limitations associated with these models, such as hallucinations, and to provide precise and contextual prompts to ensure accurate and relevant outputs. As the field of LLMs continues to evolve, it is likely that we will see significant advancements in the development of more sophisticated and accurate language models.\n"
          ]
        }
      ],
      "source": [
        "print(chain.invoke({\"question\":\"Give an essay on LLMs\"}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "XtUwmAtJCGbP",
        "outputId": "5ab3bc67-9867-4ce2-bf79-fc5593246280",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are 5 descriptive questions on LLMs with answers based on the provided context:\n",
            "\n",
            "1. **What are Large Language Models (LLMs), and how do they work?**\n",
            "Answer: LLMs are a sophisticated type of neural network that process and generate text. They are trained on extensive textual data, enabling them to grasp various language patterns and structures. The primary goal of LLMs is to interpret and create human-like text that captures the nuances of natural language.\n",
            "\n",
            "2. **What is the core training objective of LLMs, and what emergent abilities have they demonstrated?**\n",
            "Answer: The core training objective of LLMs focuses on predicting the next word in a sentence. This straightforward objective has led to the development of emergent abilities, such as conducting arithmetic calculations, unscrambling words, and demonstrating proficiency in professional exams, like passing the US Medical Licensing Exam.\n",
            "\n",
            "3. **How do LLMs generate text, and what is the role of the attention mechanism in this process?**\n",
            "Answer: The text generation process in LLMs is autoregressive, meaning they generate the next tokens based on the sequence of tokens already generated. The attention mechanism is a vital component in this process, establishing word connections and ensuring the text is coherent and contextually appropriate.\n",
            "\n",
            "4. **What are prompts in the context of LLMs, and how can they be optimized for effective results?**\n",
            "Answer: Prompts are instructions given to LLMs, providing context to generate human-like text. To optimize prompts, it's essential to use precise language, provide sufficient context, test variations, and review outputs for accuracy and relevance. Concise, descriptive, and short prompts generally lead to more effective results.\n",
            "\n",
            "5. **What are hallucinations in LLMs, and why are they a concern?**\n",
            "Answer: Hallucinations in LLMs refer to instances where the model produces outputs inconsistent with facts or available inputs. This can lead to the spread of disinformation, especially in crucial industries. Hallucinations occur when the model creates outputs that do not correspond to real-world facts or context, highlighting the need for careful review and validation of LLM-generated content.\n"
          ]
        }
      ],
      "source": [
        "print(chain.invoke({\"question\":\"Give 5 descriptive questions on LLMs with answers\"}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Nfnnab1WCGbP"
      },
      "outputs": [],
      "source": [
        "loader=PyPDFLoader(\"VITEEE_Brochure.pdf\")\n",
        "pages=loader.load_and_split()\n",
        "\n",
        "vectorstore = DocArrayInMemorySearch.from_documents(\n",
        "    pages,\n",
        "    embedding=embedding_function\n",
        ")\n",
        "\n",
        "retriever=vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ZyYD0sc3CGbQ",
        "outputId": "76c36118-50de-417e-9dfd-7d418cb78c76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "retriever.invoke(\"Machine Learning\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "bwqWu34ACGbQ",
        "outputId": "f3a973e6-add5-4cf6-d3af-d112bc9e9a9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "retriever.invoke(\"VITEEE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "H1iCSxieCGbQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1be41175-efe5-410f-c423-79558c7a6c8d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 0, 'page_label': '1'}, page_content='/vituniversity /vellore_vit www.vit.ac.in /vellore-institute-of-technology/VIT_univ\\n2024\\n2024\\nVIT ENGINEERING ENTRANCE\\nEXAMINATION\\nVIT ENGINEERING ENTRANCE\\nEXAMINATION\\nFor Admission to B.T ech. Programmes of\\nVIT - Vellore | VIT - Chennai | VIT - AP | VIT - Bhopal\\nFor Admission to B.T ech. Programmes of\\nVIT - Vellore | VIT - Chennai | VIT - AP | VIT - Bhopal\\nciogNg caHT jUk\\nVELLORE INSTITUTE OF TECHNOLOGY\\nVIT\\nVIT\\nVellore Institute of Technology\\n(Deemed to be University under section 3 of UGC Act, 1956)\\nR\\nVITEEE\\nProspectus'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 1, 'page_label': '2'}, page_content='VIT - Vellore\\nVIT - Chennai\\nVIT - AP\\nVIT - Bhopal'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 2, 'page_label': '3'}, page_content='1.  About VIT\\n2.     B. Tech Programmes Oﬀered \\n3.     Online Application Form \\n4.     Eligibility Criteria\\n5.     Admission Process\\n6.      About VITEEE\\n6.1    Test Cities \\n7.     Selection Criteria\\n8.     Counselling Process \\n9.     Admission Process for Candidates under NRI Category \\n10.    Document Submission\\n11.    Scholarship\\n12.    Fee Structure\\n13.    Withdrawal Procedure\\n14.    Refund Policy\\n15.    Campus life\\n16.     Hostels   \\n17.    Anti ragging committee\\n18.    Important Dates\\n01\\n01\\n03\\n05\\n07\\n07\\n08\\n09\\n09\\n10\\n11\\n11\\n12\\n13\\n13\\n14\\n14\\n15\\n15\\nContents'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 3, 'page_label': '4'}, page_content='VIT (Vellore Institute of Technology), established in 1984, was conferred with the status of a deemed university in 2001 by the Govern-\\nment of India. In three decades, the Institution has emerged as an innovator in higher education, growing from 180 students to 67,000+ \\nstudents. Today, it is counted among the most prominent and respected universities in India, featuring top-ranked academic \\nprogrammes. The main campus of VIT is at Vellore. It started another campus in Chennai subsequently VIT - AP at Amaravati and VIT - \\nBhopal at Madhya Pradesh were started as State Private Universities later. Eco-friendly in design and equipped with cutting-edge \\ninfrastructure, VIT provides a favorable atmosphere for learning and living. Quality in teaching-learning, research, and innovation are at \\nthe core of education at VIT. The institution follows a student-centered culture and places the highest importance on quality and consis -\\ntency in learning, teaching, and research experiences.\\nAbout VIT1.\\nB.Tech - Aerospace Engineering\\nVellore Chennai AP Bhopal\\nB.Tech - Biotechnology\\nVellore Chennai AP Bhopal\\nB.Tech - Bioengineering\\nVellore Chennai AP Bhopal\\nB.Tech - Chemical Engineering\\nVellore Chennai AP Bhopal\\nB.Tech - Civil Engineering\\nVellore Chennai AP Bhopal Vellore Chennai AP Bhopal\\nVellore Chennai AP BhopalVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nB.Tech - Computer Science and Engineering\\n(Artiﬁcial Intelligence and Robotics)\\nB.Tech - Computer Science & Engineering\\n(Cyber Security & Digital Forensics)\\nB.Tech - Computer Science and Engineering\\n(Health Informatics)\\nB.Tech - Computer Science & Engineering\\n(Cloud Computing & Automation)\\nB.Tech - Computer Science and Engineering\\n(Bioinformatics)\\nB.Tech - Computer Science and Engineering and\\nBusiness systems (in collaboration with TCS)\\nB.Tech - Computer Science and Engineering\\n(Information Security)\\nB.Tech - Computer Science and Engineering\\nand Business Systems\\nB.Tech - Computer Science and Engineering\\n(Data Science)\\nB.Tech - Computer Science and Engineering\\n(Internet of Things)\\nB.Tech - Computer Science and Engineering\\n(Cyber Physical Systems)\\nB.Tech - Computer Science and Engineering\\nB.Tech - Computer Science and Engineering\\n(Block Chain Technology)\\n01\\n03\\n05\\n07\\n10\\n12\\n14\\n16\\n02\\n04\\n06\\n08\\n09\\n11\\n13\\n15\\n17 18\\n2. B.Tech. Programmes Oﬀered\\n1'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 4, 'page_label': '5'}, page_content='Vellore Chennai AP Bhopal\\nB.Tech - Computer Science & Engineering\\n(Gaming Technology)\\nVellore Chennai AP BhopalVellore Chennai AP Bhopal\\nVellore Chennai AP BhopalVellore Chennai AP Bhopal\\nVellore Chennai AP BhopalVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nB.Tech - Electronics and Instrumentation\\nEngineering\\nB.Tech - Electronics and Computer EngineeringB.Tech - Electrical and Computer Science\\nEngineering\\nB.Tech - Fashion Technology\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP BhopalVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nB.Tech - Computer Science & Engineering\\n(E-Commerce Technology)\\nB.Tech - Electronics & Communication\\nEngineering (Artiﬁcial Intelligence & Cybernetics)\\nB.Tech - Electronics Engineering\\n(VLSI Design and Technology)\\nB.Tech - Electrical and Electronics Engineering\\nB.Tech - Electronics & Communication\\nEngineering\\nB.Tech - Electronics and Communication\\nEngineering (Biomedical Engineering)\\nB.Tech - Mechanical Engineering\\n(Artiﬁcial Intelligence & Robotics)\\nB.Tech - Information Technology\\nB.Tech - Computer Science and Engineering\\n(Cyber Security)\\nB.Tech - Mechanical Engineering\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nB.Tech - Mechatronics and Automation\\nB.Tech - Computer Science & Engineering\\n(Education Technology)\\nB.Tech - Mechanical(Manufacturing Engineering)\\n19 20\\n21\\nVellore Chennai AP Bhopal\\nB.Tech - Computer Science and Engineering\\n(Blockchain)\\n23\\n28\\nVellore Chennai AP Bhopal\\nB.Tech - Electronics and Communication\\nEngineering (Embedded Systems)\\n30\\nVellore Chennai AP Bhopal\\nB.Tech - Electronics and Communication\\nEngineering (VLSI)\\n31\\n27\\n32\\n29\\n3433\\n3635\\n3837\\n39\\n24\\nVellore Chennai AP Bhopal\\nB.Tech - Computer Science and Engineering\\n(Software Engineering)\\nVellore Chennai AP Bhopal\\nB.Tech - Computer Science and Engineering\\n(Data Analytics)\\n25 26\\n40\\n45\\n43\\nVellore Chennai AP Bhopal\\nB.Tech - Mechanical Engineering\\n(Electric Vehicles)\\nVellore Chennai AP Bhopal\\nB.Tech - Mechanical Engineering\\n(Automotive Design)\\n44\\n41\\nVellore Chennai AP Bhopal\\nB.Tech - Mechanical Engineering\\n(Robotics)\\n42\\n2\\nVisit https://vitap.ac.in/btech/ for full list of specialisation / Minor / Double Major options\\nVellore Chennai AP Bhopal\\nB.Tech - Computer Science & Engineering\\n(Artiﬁcial Intelligence & Machine Learning)\\n22'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 5, 'page_label': '6'}, page_content='3.1. Guidelines for ﬁlling up the application form\\nThe B.Tech Admission Application Form is available online at www.viteee.vit.ac.in. The Cost of the Application Form is Rs. 1,350 /- \\n(Non-Refundable).\\nValid e-mail address and active mobile number are required.\\nEnter all the relevant details carefully.\\nAn OTP will be sent to your Registered Indian Mobile Number for veriﬁcation.\\nOnce OTP is validated, a password will be sent to your registered email address.\\nAll the correspondence shall be made to the registered email address and mobile number \\nonly.\\n3.1.1. New User Registration\\nValid e-mail address and active mobile number are required.\\n3.1.2. Sign-in for Registered Users\\nFill in the relevant ﬁelds carefully. An Application Number will be generated after ﬁlling \\nprimary details.\\nRefer this application number in all your future correspondence.\\n3.1.3. Filling online application form\\nApplication cost of Rs.1350 should be paid through Net Banking Account/ Credit/ \\nDebit Card/ Paytm\\nFor Abroad test centre : Equivalent to USD 90 in INR\\nNote : Application cost is non-refundable\\n3.1.4. Payment\\nStep\\n04\\nStep\\n03\\nStep\\n02\\nStep\\n01\\n3. Online Application Form\\n3'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 6, 'page_label': '7'}, page_content='The photograph must be taken against a light background. Only White/oﬀ-white \\nbackground is preferred.\\nThe photograph must show the full frontal view without tilting of face, both ears visible, \\nopen eyes with neutral expression.\\nMake sure there is no glare from spectacles and no blur in the focus.\\nThe ﬁle should be only in JPEG format.\\nThe ﬁle size of the photograph should be within 20 kB to 300 kB.\\nThe photograph dimension should be of Width(300 to 400 pixels) X Height (400 to 550 \\npixels).\\nThe photograph should not be too dark or too bright but should have optimum exposure.\\nThe photograph should have only one face.\\nPlease draw a rectangular box of size 6 cm (width) X 2 cm (height) on a A4 white paper. Sign \\nwith dark blue or black pen within the box.\\nScan the signature in (.jpeg) format using scanner and crop the image.\\nYou can open the scanned signature image with MS Paint and crop the image at the \\nparticular rectangular box which was already marked and signed on the A4 white paper.\\nWhile cropping the image make sure that the dimension should be 3.5 cm (width) X 1.5 cm \\n(height).\\nThe scanned signature image size should be within 20 kB to 300 kB.\\nThe photograph must be in color and should be taken professionally in a studio with the \\nfollowing speciﬁcations:\\n3.1.5. Photo and Signature Upload\\n3.1.6. Download ﬁlled-in application form\\nDownload and save the ﬁlled-in application for your reference and further \\ncorrespondence.\\nApplicants NEED NOT SEND hard copy of the ﬁlled-in application form to VIT.\\nSignature uploading procedure:\\nStep\\n05\\nStep\\n06\\n4'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 7, 'page_label': '8'}, page_content=\"4.1. Nationality\\nThe applicant should be a Resident / Non-Resident Indian National / PIO / OCI.\\nNRI applicants can directly apply under 'NRI Category' through NRI application form. \\nForeign applicants who studied/studying abroad can apply directly through the International application \\nform. (https://vit.ac.in/admissions/international/overview)\\nApplicants whose date of birth falls on or after 1st July 2002 are eligible to apply for engineering admission \\n2024.\\nThe date of birth as recorded in the High School / SSC / X Certiﬁcate will be considered authentic.\\nApplicants should produce this certiﬁcate in original as proof of their age at the time of \\ncounselling/admission, failing which their candidacy for admission will be disqualiﬁed.\\n4.2. Age Limit\\n4.3. Qualifying Examination\\nThe ﬁnal examination of the 10+2 system of higher secondary examination conducted by the State Board; \\nCentral Board of Secondary Education (CBSE, New Delhi), The Council for Indian School Certiﬁcate \\nExamination (ISCE),New Delhi.\\nIntermediate or Two-year Pre-University Examination conducted by a recognized Board/ University.\\nHigh School Certiﬁcate Examination of the Cambridge University or International Baccalaureate Diploma of \\nthe International Baccalaureate Oﬃce, Geneva. (Physics & Mathematics - HL, Chemistry - SL)\\nGeneral Certiﬁcate Education (GCE) examination (London/Cambridge/Srilanka) at the Advanced (A) level.\\nRegular 'NIOS' board candidates are also eligible for Undergraduate Engineering programmes. Applicants \\nshould produce the Migration Cum Transfer Certiﬁcate at the time of joining.\\nApplicants who have completed the Class 12 (or equivalent) examination outside India or from a Board not \\nspeciﬁed above should produce a certiﬁcate from the Association of Indian Universities (AIU) to the eﬀect \\nthat the examination they have passed is equivalent to the Class 12 Examination with grade/CGPA converted \\nto percentage.\\nApplicants applying for UG Engineering admission should have either completed or shall be appearing in \\n2024, in any one of the following qualifying examinations:\\nFor Indian nationals attending VITEEE 2024 is mandatory to be eligible for B. Tech. admission.\\nIn case VITEEE 2024 gets cancelled due to natural calamities, qualifying criteria may be substituted with higher \\nsecondary marks or JEE-24 or a valid SAT. Applicants will be informed with speciﬁcs in such case.\\n4. Eligibility Criteria\\n4.1.1.\\n4.1.2.\\n4.1.3.\\n4.2.1.\\n4.2.2.\\n4.2.3.\\n4.3.1.\\n4.3.2.\\n4.3.3.\\n4.3.4.\\n4.3.5.\\n4.3.6.\\n5\"),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 8, 'page_label': '9'}, page_content='Applicants applying for the Undergraduate Engineering admission in 2024 should have secured minimum \\naggregate of 60% in Physics, Chemistry, and Mathematics/Biology (PCM/PCB/PCMB) in the qualifying examination \\n(+2/Intermediate).\\nApplicants applying for the Undergraduate Engineering admission in 2024 should have passed with minimum \\naggregate of 50% in Physics, Chemistry, and Mathematics/Biology in the qualifying examination (+2/Intermediate) \\nfor the following categories:\\n Applicants belonging to SC/ST\\n Applicants hailing from Jammu and Kashmir/ Ladakh and the Northeastern states of Arunachal Pradesh, \\nAssam, Manipur, Meghalaya, Mizoram, Nagaland, Sikkim and Tripura. Certiﬁcate to prove community / nativity \\nshould be produced at the time of counselling, failing which they will not be considered for admission.\\nPCMB/PCM applicants who have attempted Mathematics, Physics, Chemistry, English and Aptitude (MPCEA) in \\nVITEEE 2024 are eligible for all B. Tech Programmes.\\nPCMB/PCB applicants who have attempted Biology, Physics, Chemistry, English and Aptitude (BPCEA) in VITEEE \\n2024 & PCB applicants who have attempted Mathematics, Physics, Chemistry, English and Aptitude (MPCEA) in \\nVITEEE 2024 are eligible only for the following programmes :\\n4.4.  Requirement of Qualifying Examination\\nB.Tech. Biotechnology (VIT, Vellore)\\nB.Tech. Computer Science and Engineering (Bioinformatics), (VIT - Vellore)\\nB.Tech. Electronics and Communication Engineering (Biomedical Engineering), (VIT - Vellore)\\nB.Tech. Bioengineering (VIT - Bhopal)\\nB.Tech. Computer Science and Engineering (Health Informatics), (VIT - Bhopal)\\n4.4.1.\\n4.4.2.\\n4.5.1.\\n4.5.2.\\n1.\\n2.\\n3.\\n4.\\n5.\\n4.5. Subject Eligibility\\n6'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 9, 'page_label': '10'}, page_content='Submission of VITEEE\\nApplication through\\nWeb by\\napplicants\\nhttps://viteee.vit.ac.in \\nGenerate link for\\ntest slot booking\\nto the candidates\\nApplication\\nVeriﬁcation\\n&\\nScrutiny\\nAppear for\\nVITEEE 2024\\nAnnouncement of\\nOnline Counselling\\nbased on\\nVITEEE 2024 Rank\\nTest Slot,\\nCentre booking &\\nAdmit Card\\nGeneration\\nby the candidates\\nVITEEE 2024\\nResult\\nAnnouncement\\nProvisional\\nAdmission Letter\\nto B.Tech Programme\\nbased on Merit after\\nrequired fee paymentDocument\\nVeriﬁcation\\nProgramme wishlist\\nby the candidates\\n Vellore Institute of Technology Engineering Entrance Examination (VITEEE) is conducted for admission to undergraduate \\nengineering programmes in VIT group of Institutions. VITEEE will be conducted between 19 and 30 April 2024 (Tentative) at designat-\\ned centres across India and abroad (No.of days will vary for test cities). The duration of the examination will be 2 hours and 30 \\nminutes. Candidates can appear only once for VITEEE. All questions will be Multiple Choice Questions and one mark for the right \\nanswer and zero for the wrong answer. There will be a total of 125 questions divided into the section Maths/Biology (40 questions), \\nPhysics (35 questions), Chemistry (35 questions), Aptitude (10 questions), and English (5 questions). The question paper will be in \\nEnglish only. The fee towards application and subsequent counselling is non-refundable. The detailed syllabus is available at www.vi-\\nteee.vit.ac.in. The method of Equi-percentile is adopted for the merit list preparation of VITEEE-Computer Based Examination.\\n6. About VITEEE\\n5. Admission Process\\n7\\n01\\n03\\n04\\n05\\n06\\n07\\n08\\n09\\n10\\n02'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 10, 'page_label': '11'}, page_content='Imphal \\nAizawl\\nAgartala\\nShillong\\nAmaravati(AP) | Ananthapur | Eluru | Kurnool | Nellore\\nRajahmundry | Srikakulam | Tanuku | Tirupati | Vishakapatnam\\nAhmedabad | Rajkot | Surat\\nVadodara | Jamnagar\\nFaridabad | Gurgaon | Hissar \\nKurukshetra\\nDharmasala | Shimla\\nJammu | Srinagar\\nBengaluru | Hubli | Mangalore | Mysuru\\nKochi | Kozhikode | Thiruvananthapuram\\nThrissur\\nAmaravati | Aurangabad | Latur | Mumbai\\nNagpur | Nashik | Pune | Solapur | Thane\\nAssagao\\nPuducherry \\nAmrister | Bathinda | Jalandhar City\\nLudhiana \\nAjmer | Bikaner | Jaipur\\nJodhpur | Kota | Udaipur \\nHyderabad | Karim Nagar | Khammam \\nMahbubnagar | Nalgonda | Warangal\\nChennai | Coimbatore | Erode | Kancheepuram | Kumbakonam \\nMadurai | Nagarcoil | Salem | Tiruchirapalli | Tirunelveli | Vellore\\nChandigarh\\nDelhi\\nBhopal | Gwalior | Jabalpur\\nJAMMU & KASHMIR\\nHIMACHAL PRADESH\\nPUNJAB\\nCHANDIGARH\\nHARYANA\\nDELHI\\nRAJASTHAN\\nMADHYA PRADESH\\nGUJARAT\\nMAHARASHTRA\\nGOA\\nKARNATAKA\\nKERALA\\nTELANGANA\\nTAMILNADU\\nPUDUCHERRY\\nANDHRA PRADESH\\nPort Blair\\nItanagar\\nDibrugar\\nGuwahati | Silchar\\nBokaro | Dhanbad\\nJamshedpur | Ranchi\\nDimapur \\nBerhampur\\nBhubaneswar | Rourkela\\nGangtok\\nDehradun | Pant Nagar | Roorkee\\nDurgapur | Kolkata\\nSiliguri\\nDubai | Kuwait | Muscat | Qatar | Kuala Lumpur | Singapore\\nAgra | Aligarh | Prayagraj | Bareilly | Ghaziabad | Gorakhpur | Jhansi | Kanpur | Lucknow\\nMathura | Meerut | Moradabad | Noida | Rae Bareli | Saharanpur | Varanasi\\nBhagalpur | Gaya | Muzaffarpur | Patna\\nBhilai | Bhilaspur | Raipur\\nUTTARAKHAND\\nUTTAR PRADESH\\nBIHAR\\nSIKKIM\\nARUNACHAL PRADESH\\nASSAM\\nNAGALAND\\nMANIPUR\\nMIZORAM\\nTRIPURA\\nMEGHALAYA\\nWEST BENGAL\\nJHARKHAND\\nODISHA\\nCHATTISGARH\\nANDAMAN & NICOBAR ISLANDS\\nCENTRES ABROAD\\n6.1. Test Cities\\n8'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 11, 'page_label': '12'}, page_content='Selection is based on the rank secured through CBT (Computer Based Test) VITEEE 2024.\\nSelected candidates can participate in online counselling based on their ranking. They can give options for speciﬁc \\ncampus, programme and category of fees. Allotment will be based on the rank obtained, preference given and \\navailability during online counselling.\\nCandidates with rank Upto 1 Lakh are eligible for counselling to all the four campuses, VIT - Vellore, VIT - Chennai, VIT - \\nAP and VIT - Bhopal. Rank holders above 1 lakh are eligible for counselling to VIT - AP and VIT - Bhopal campuses only.\\nVIT will verify the eligibility of candidates after completing the tuition fee payment on or before the stipulated date. \\nCandidates are required to produce documents on the day of reporting to the campus or as and when asked for, by the \\nOﬃce of Admissions. Provisional Admission oﬀered during counselling will be valid ONLY if the eligibility is fulﬁlled.\\nThe counselling process is common for VIT group of Institutions. The online counselling will be held in ﬁve phases. The \\nrank, schedule, choice entry will be available in the counselling portal.\\nA non-refundable counselling fee of Rs.5,900/- is required to participate in the counselling process.\\nThe applicants who fail to appear in their phase of counselling are eligible to appear in next phase, but limited to \\navailable choices in that phase. \\nApplicants may select campus / programme / fee category. The allotment will be based on VITEEE Rank. Once the \\nrequired fee is paid, provisional admission letter will be available in portal.\\nOnce allotted campus / programme / fee category cannot be changed.\\n \\n7.\\n7.1.\\n7.2.\\n7.3.\\n7.4.\\n8.1.\\n8.2.\\n8.3.\\n8.4.\\n8.5.\\nSelection Criteria\\n8. Online Counselling Process\\n9'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 12, 'page_label': '13'}, page_content='There is no Entrance Examination. Admission is based on merit (marks/grades obtained in the \\nqualifying examination-refer the                                 ) and availability of seats.\\nInterested NRI candidates can also apply through the given link : https://viteee.vit.ac.in/\\nEligibility Norms\\nProof of Age & Nationality (Passport/Birth Certiﬁcate)\\n11th grade marksheet/AS Level/Form V(for appearing candidates) & 12th grade mark sheet/A Level/Form VI (for \\npassed out candidates)\\nVisa/Resident Card authenticating NRI status of parent\\nEmployer certiﬁcate (Optional at the time of application)\\nOn satisfying all the eligibility requirements, the branch allotment order will be e-mailed to the registered email \\nid (candidates are requested to give valid email address of your parent)\\nThe provisional admission letter will be issued along with the fees structure. (only if the students meet the \\neligibility requirements)\\nAllotment of branch will be done on the basis of the application and documents received at our end\\nBranch once allotted will not be changed under any circumstances\\nSelected candidates are required to pay the Fees in USD as mentioned on the oﬀer letter\\n9.1. Scrutinizing the documents received from the candidate.\\nStep\\n02\\nStep\\n03\\nStep\\n01\\n9.2. Issue of oﬀer letter\\n9.3. Payment of Advance Fees\\nStep\\n04 9.4. Payment of Balance Fees\\nStep\\n05 9.5. Intimation on other Admission Formalities\\nStep\\n06 9.6. Payment of Hostel Fees\\nNote: The ﬁlled-out application form along with necessary enclosures are to be duly uploaded along with the \\nform.\\nThe hostel fees should be paid seperately along with the tuition fees. Refer the link below for the fee structure.\\nThe candidate is requested to pay the balance Tuition fee as per the provisional admission letter along with the \\ncopy of the XII mark statement / website copy [if available ] / Copy of XII Hall Ticket or Admit card through Fund \\nTransfer mode in USD.\\nOn receipt of the balance fee, the details regarding the commencement of the programme and document \\nsubmission will be intimated through email.\\nCandidates are requested to report to the University with the required original documents as per the provisional \\nadmission letter.\\n9. Admission Process for candidate under NRI Category\\nVIT - Vellore VIT - Chennai VIT - BhopalVIT - AP\\n10'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 13, 'page_label': '14'}, page_content='Copy of Provisional Letter\\nXII Mark statements of all attempts of qualifying examination\\nTransfer Certiﬁcate / School leaving Certiﬁcate\\nMigration Certiﬁcate (if issued by the board)\\nStudent Proﬁle (https://admissions.vit.ac.in/freshersportal/login) – (Signed by Student & Parent)\\nAﬃdavit by the Student (https://vit.ac.in/ﬁles/admissions/Aﬃdavit_Student.pdf) – (Signed & Notarized)\\nAﬃdavit by the Parent (https://vit.ac.in/ﬁles/admissions/Aﬃdavit_Parent.pdf) - (Signed & Notarized)\\nHostel Aﬃdavit (https://vit.ac.in/campuslife/hostels) - (Signed by Notarized)\\nCertiﬁcate of Physical Fitness (https://vit.ac.in/ﬁles/admissions/PhysicalFitness_Certiﬁcate.pdf)- (Certiﬁed by authorized \\nPhysician)\\nUndertaking form (https://vit.ac.in/sites/default/ﬁles/Undertaking.pdf) - (Signed by Student & Parent)\\nRecent passport size colour photos – 2 Nos.\\nAadhar card / Age proof certiﬁcate\\nX & XII mark sheet\\nTransfer Certiﬁcate/School Leaving Certiﬁcate\\nMigration Certiﬁcate\\nCommunity Certiﬁcate (For SC/ST only)\\nNativity Certiﬁcate (For candidates hailing from Jammu and Kashmir, Ladhak and the North Eastern states)\\nPassport Copy (Student & Parent)\\nEmployer Certiﬁcate (original)\\nSponsorship letter (original)\\nNote: All copies of documents will be archived and will not be returned for any reason.\\nOriginal Documents:\\nCopy of documents (1 set) :\\nAdditional Documents required from NRI/Foreign students :\\n*The Scholarship will be realized from second semester\\n10. Document Submission on Admission\\n11. Scholarship*\\n11\\nVITEEE rank holders of 1 to 10\\nVITEEE rank holders of 11 to 50\\nVITEEE rank holders of 51 to 100\\nVITEEE rank holders of 101 to 500\\n75% Tuition fee waiver for all\\nthe four years\\n50% Tuition fee waiver for all\\nthe four years\\n25% Tuition fee waiver for all\\nthe four years\\n100% Tuition fee waiver for all\\nthe four years'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 14, 'page_label': '15'}, page_content='B.TECH.- Bioengineering\\nB.TECH.- Biotechnology\\nB.TECH.- Chemical Engineering\\nB.TECH.- Civil Engineering\\nB.TECH.- Electrical and Electronics Engineering\\nB.TECH.- Electronics and Instrumentation Engineering\\nB.TECH.- Fashion Technology\\nB.TECH.- Aerospace Engineering\\nB.TECH.- Computer Science & Engineering (E-Commerce Technology)\\nB.TECH.- Computer Science & Engineering (Education Technology)\\nB.TECH.- Computer Science and Engineering and Business Systems\\nB.TECH.- Computer Science and Engineering\\nB.TECH.- Computer Science and Engineering (Artiﬁcial Intelligence and Machine Learning)\\nB.TECH.- Computer Science and Engineering (Artiﬁcial Intelligence and Robotics)\\nB.TECH.- Computer Science and Engineering (Bioinformatics)\\nB.TECH.- Computer Science and Engineering (Block Chain Technology)\\nB.TECH.- Computer Science and Engineering (Block Chain)\\nB.TECH.- Computer Science and Engineering (Cyber Physical Systems)\\nB.TECH.- Computer Science and Engineering (Cyber Security)\\nB.TECH.- Computer Science and Engineering (Data Science)\\nB.TECH.- Computer Science and Engineering (Data Analytics)\\nB.TECH.- Computer Science and Engineering (Information Security)\\nB.TECH.- Computer Science and Engineering (Internet of Things)\\nB.TECH.- Computer Science & Engineering (Cloud Computing & Automation)\\nB.TECH.- Computer Science & Engineering (Health Informatics)\\nB.TECH.- Computer Science and Engineering and Business Systems (in collaboration with TCS)\\nB.TECH.- Computer Science & Engineering (Cyber Security & Digital Forensics)\\nB.TECH.- Computer Science & Engineering (Gaming Technology)\\nB.TECH.- Computer Science and Engineering (Software Engineering)\\nB.TECH.- Electrical and Computer Science Engineering\\nB.TECH.- Electronics and Communication Engineering\\nB.TECH.- Electronics and Communication Engineering (Biomedical Engineering)\\nB.TECH.- Electronics & Communication Engineering (Artiﬁcial Intelligence & Cybernetics)\\nB.TECH.- Electronics and Communication Engineering (Embedded Systems)\\nB.TECH.- Electronics and Communication Engineering (VLSI)\\nB.TECH.- Electronics and Computer Engineering\\nB.TECH.- Electronics Engineering (VLSI Design and Technology)\\nB.TECH.- Information Technology\\nB.TECH.- Mechanical Engineering\\nB.TECH.- Mechanical Engineering (Automotive Design)\\nB.TECH.- Mechanical Engineering (Electric Vehicles)\\nB.TECH.- Mechanical Engineering (Manufacturing Engineering)\\nB.TECH.- Mechanical Engineering (Artiﬁcial Intelligence & Robotics)\\nB.TECH.- Mechanical Engineering (Robotics)\\nB.TECH.- Mechatronics and Automation\\n(*after concession)\\nGroup - B\\nGroup - A\\nFee Stucture :\\nTuition Fees (Per Annum)\\nCaution Deposit (Refundable) (One time payment)\\nTotal Fees to be paid for the ﬁrst year\\nINR 1,95,000*\\nINR 3,000\\nINR 1,98,000\\n(*after concession)\\nFee Stucture :\\nTuition Fees (Per Annum)\\nCaution Deposit (Refundable) (One time payment)\\nTotal Fees to be paid for the ﬁrst year\\nINR 1,73,000*\\nINR 3,000\\nINR 1,76,000\\n12. Fee Structure\\n12'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 15, 'page_label': '16'}, page_content='1\\n2\\n3\\nGroup - B\\nOnline withdraw / Submit in person\\nNecessary form for withdrawal / Clearances should be obtained to enable withdrawal.\\nDate of Application for withdrawal is on the date of submitting clearance form.\\n(*)Includes any kind of leave / absent to the class | *GST if any applicable will be deducted in addition\\nNote: The policy announced by AICTE / UGC, will be adopted, as and when announced. \\n13. Withdrawal Procedure\\n14. Refund Policy\\nConditionsSl.\\nNo.\\nFor Not Eligible Candidate\\n(Either fail or Less than 60% in PCM / PCB\\nRs.1000/- +GST 18% will be\\ndeducted = Total: Rs. 1,180/-\\n250$ + GST 18% (45$) will be\\ndeducted = Total: 295$\\n2\\nFor withdraw request *received before the\\ncommencement of the programme\\n(Orientation / last date of admission /\\nClasses whichever is earlier)\\nRs.1000/- +GST 18% will be\\ndeducted = Total: Rs. 1,180/-\\n1000 $ + GST 18% (180$) will\\nbe deducted = Total: 1,180 $\\n3\\nFor Withdraw request *received within 25\\ndays from the commencement of the\\nprogramme (Orientation / last date of\\nadmission / Classes whichever is earlier)\\n10% of the academic fees will be\\ndeducted and balance amount\\nwill be refunded\\n2000 $ will be deducted*\\n4\\nFor Withdraw request *received after 25\\ndays within 40 days from the commencement\\nof the programme (Orientation / last date of\\nadmission / Classes whichever is earlier)\\n20% of the academic fees will be\\ndeducted and balance amount\\nwill be refunded\\n3000 $ will be deducted*\\n5\\nFor Withdraw request *received after 40\\ndays from the commencement of the\\nprogramme (Orientation / last date of\\nadmission / Classes whichever is earlier)\\nOnly caution deposit will be\\nrefunded.\\nOnly caution deposit will be\\nrefunded.*\\nRefund Rule\\n(Normal Category)\\nRefund Rule\\n(NRI / Foreign Category)\\n1\\n13'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 16, 'page_label': '17'}, page_content='Every attempt has been done by the management and other administrative boards to assure that every student ﬁnds VIT a very \\nlively, fun and resourceful community to employ their erudite years. Conscious of the inﬂuence these active years can have on the minds of \\nthe expectation of our nation and the globe, VIT endeavours to promote, introduce and expand any and all ventures to shape their minds. \\nBy bringing clubs, chapters and college festivals, students are not simply revealed to a competitive environment inside the university but also \\nwith reputed universities and colleges in and around the country.\\n A home away from home, the hostels at VIT is more than four walls and a roof. They have been designed to provide a comfortable, \\nsafe, inclusive and secure living even as they provide opportunities to form lasting friendships and ease the transition from home to college. \\nResidential staﬀ is always available to support the students from diﬀerent backgrounds to promote community. Living on campus provides \\nstudents with opportunities to:\\nDevelop stronger interpersonal and communication skills\\nEstablish relationships with faculty and staﬀ members\\nConnect with a diverse population of people\\nEngage in campus leadership, organizations and activities\\n15. Campus life\\n16. Hostels\\nRefer the link below for the respective campus life\\nVIT - Vellore VIT - Chennai VIT - AP VIT - Bhopal\\nRefer the link below for the respective hostels\\nVIT - Vellore VIT - Chennai VIT - AP VIT - Bhopal\\n14'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 17, 'page_label': '18'}, page_content='Last date for Receipt of the Application  : March 31, 2024 (Tentative)\\nVITEEE      : April 19 - 30, 2024 (Tentative)\\n(Number of days will vary for test cities)\\nResult Declaration    : 3rd May, 2024 (Tentatively)\\nCounselling     : To be announced\\nLast date of Admission    : 7th July, 2024\\n VIT gives immense emphasis on maintaining a ragging free campus. Anti-Ragging Committee is constituted with Director, \\nStudents’ Welfare as Chairperson, Assistant Directors, Students’ Welfare as Members and nominated Faculty members from each \\nschool as Coordinators. Along with the Coordinators from each school, there is a team of squad members that comprises of faculty \\nas well as staﬀ members.\\n If any student has any grievance in the nature of ragging, including any kind of harassment, the same can be brought to \\nthe notice of the Chairperson or any of the Committee Members from various schools, for necessary action and redressal.\\n17. Anti ragging committee\\n18. Important Dates\\n17.1.\\n17.2.\\n15'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 18, 'page_label': '19'}, page_content='vellore_vit  vellore-institute-of-technology   vituniversity  vit_univ\\nDirector (UG Admission)\\nVellore Institute of Technology\\nVellore 632 014, Tamilnadu, India.\\nPhone No.  :  044 4627 7555\\nEmail Address : ugadmission@vit.ac.in\\nWebsite : www.vit.ac.in \\nVIT KOTA Information Centre\\n117, Near Nucleus Coaching,\\nOld Rajiv Gandhi Nagar,\\nKota (Rajasthan) 324005\\nContact No. : 9782376476, 7690074787\\nEmail: aadilnaki.pathan@vit.ac.in\\nWebsite : www.vit.ac.in\\nAccreditation & Ranking\\nPlacement (2023 Graduated Batch)\\nFor More Details\\n16\\nLegal Jurisdiction : All suits and actions arising out of or relating to VIT shall be instituted within the Jurisdiction of courts at Vellore, Tamilnadu only.\\n947\\nNo. of\\nCompanies\\n4,493\\nNo. of Super\\nDream Oﬀers\\n3,540\\n14,565\\nNo. of\\nDream Oﬀers\\n6,532\\nNo. of\\nRegular Oﬀers\\nOverall\\nOﬀers\\n1.02 Cr.\\nHighest\\nPackage (Per Annum)\\nIncludes Internships & Placements (10LPA & Above)\\nAccredited with the highest\\ngrade A++ by NAAC\\nRanked #11\\nBest Engineering Institution\\nof India\\nRanked #163 in Asia by\\nQS World University\\nRanking - Asia, 2024.'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 19, 'page_label': '20'}, page_content='About VIT\\n17')]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "loader=PyPDFLoader(\"VITEEE-2024-information-brochure.pdf\")\n",
        "pages=loader.load_and_split()\n",
        "\n",
        "vectorstore = DocArrayInMemorySearch.from_documents(\n",
        "    pages,\n",
        "    embedding=embedding_function\n",
        ")\n",
        "\n",
        "retriever=vectorstore.as_retriever()\n",
        "pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "LbwyBDYBCGbQ",
        "outputId": "472c3c7b-b50b-442c-b081-c8389f7bb312",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 9, 'page_label': '10'}, page_content='Submission of VITEEE\\nApplication through\\nWeb by\\napplicants\\nhttps://viteee.vit.ac.in \\nGenerate link for\\ntest slot booking\\nto the candidates\\nApplication\\nVeriﬁcation\\n&\\nScrutiny\\nAppear for\\nVITEEE 2024\\nAnnouncement of\\nOnline Counselling\\nbased on\\nVITEEE 2024 Rank\\nTest Slot,\\nCentre booking &\\nAdmit Card\\nGeneration\\nby the candidates\\nVITEEE 2024\\nResult\\nAnnouncement\\nProvisional\\nAdmission Letter\\nto B.Tech Programme\\nbased on Merit after\\nrequired fee paymentDocument\\nVeriﬁcation\\nProgramme wishlist\\nby the candidates\\n Vellore Institute of Technology Engineering Entrance Examination (VITEEE) is conducted for admission to undergraduate \\nengineering programmes in VIT group of Institutions. VITEEE will be conducted between 19 and 30 April 2024 (Tentative) at designat-\\ned centres across India and abroad (No.of days will vary for test cities). The duration of the examination will be 2 hours and 30 \\nminutes. Candidates can appear only once for VITEEE. All questions will be Multiple Choice Questions and one mark for the right \\nanswer and zero for the wrong answer. There will be a total of 125 questions divided into the section Maths/Biology (40 questions), \\nPhysics (35 questions), Chemistry (35 questions), Aptitude (10 questions), and English (5 questions). The question paper will be in \\nEnglish only. The fee towards application and subsequent counselling is non-refundable. The detailed syllabus is available at www.vi-\\nteee.vit.ac.in. The method of Equi-percentile is adopted for the merit list preparation of VITEEE-Computer Based Examination.\\n6. About VITEEE\\n5. Admission Process\\n7\\n01\\n03\\n04\\n05\\n06\\n07\\n08\\n09\\n10\\n02'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 19, 'page_label': '20'}, page_content='About VIT\\n17'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 1, 'page_label': '2'}, page_content='VIT - Vellore\\nVIT - Chennai\\nVIT - AP\\nVIT - Bhopal'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 2, 'page_label': '3'}, page_content='1.  About VIT\\n2.     B. Tech Programmes Oﬀered \\n3.     Online Application Form \\n4.     Eligibility Criteria\\n5.     Admission Process\\n6.      About VITEEE\\n6.1    Test Cities \\n7.     Selection Criteria\\n8.     Counselling Process \\n9.     Admission Process for Candidates under NRI Category \\n10.    Document Submission\\n11.    Scholarship\\n12.    Fee Structure\\n13.    Withdrawal Procedure\\n14.    Refund Policy\\n15.    Campus life\\n16.     Hostels   \\n17.    Anti ragging committee\\n18.    Important Dates\\n01\\n01\\n03\\n05\\n07\\n07\\n08\\n09\\n09\\n10\\n11\\n11\\n12\\n13\\n13\\n14\\n14\\n15\\n15\\nContents')]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "retriever.invoke(\"VITEEE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "soz3o6igCGbQ",
        "outputId": "f1304a16-c1ae-40cb-de4e-540dc2634169",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 19, 'page_label': '20'}, page_content='About VIT\\n17'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 1, 'page_label': '2'}, page_content='VIT - Vellore\\nVIT - Chennai\\nVIT - AP\\nVIT - Bhopal'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 3, 'page_label': '4'}, page_content='VIT (Vellore Institute of Technology), established in 1984, was conferred with the status of a deemed university in 2001 by the Govern-\\nment of India. In three decades, the Institution has emerged as an innovator in higher education, growing from 180 students to 67,000+ \\nstudents. Today, it is counted among the most prominent and respected universities in India, featuring top-ranked academic \\nprogrammes. The main campus of VIT is at Vellore. It started another campus in Chennai subsequently VIT - AP at Amaravati and VIT - \\nBhopal at Madhya Pradesh were started as State Private Universities later. Eco-friendly in design and equipped with cutting-edge \\ninfrastructure, VIT provides a favorable atmosphere for learning and living. Quality in teaching-learning, research, and innovation are at \\nthe core of education at VIT. The institution follows a student-centered culture and places the highest importance on quality and consis -\\ntency in learning, teaching, and research experiences.\\nAbout VIT1.\\nB.Tech - Aerospace Engineering\\nVellore Chennai AP Bhopal\\nB.Tech - Biotechnology\\nVellore Chennai AP Bhopal\\nB.Tech - Bioengineering\\nVellore Chennai AP Bhopal\\nB.Tech - Chemical Engineering\\nVellore Chennai AP Bhopal\\nB.Tech - Civil Engineering\\nVellore Chennai AP Bhopal Vellore Chennai AP Bhopal\\nVellore Chennai AP BhopalVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nB.Tech - Computer Science and Engineering\\n(Artiﬁcial Intelligence and Robotics)\\nB.Tech - Computer Science & Engineering\\n(Cyber Security & Digital Forensics)\\nB.Tech - Computer Science and Engineering\\n(Health Informatics)\\nB.Tech - Computer Science & Engineering\\n(Cloud Computing & Automation)\\nB.Tech - Computer Science and Engineering\\n(Bioinformatics)\\nB.Tech - Computer Science and Engineering and\\nBusiness systems (in collaboration with TCS)\\nB.Tech - Computer Science and Engineering\\n(Information Security)\\nB.Tech - Computer Science and Engineering\\nand Business Systems\\nB.Tech - Computer Science and Engineering\\n(Data Science)\\nB.Tech - Computer Science and Engineering\\n(Internet of Things)\\nB.Tech - Computer Science and Engineering\\n(Cyber Physical Systems)\\nB.Tech - Computer Science and Engineering\\nB.Tech - Computer Science and Engineering\\n(Block Chain Technology)\\n01\\n03\\n05\\n07\\n10\\n12\\n14\\n16\\n02\\n04\\n06\\n08\\n09\\n11\\n13\\n15\\n17 18\\n2. B.Tech. Programmes Oﬀered\\n1'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 18, 'page_label': '19'}, page_content='vellore_vit  vellore-institute-of-technology   vituniversity  vit_univ\\nDirector (UG Admission)\\nVellore Institute of Technology\\nVellore 632 014, Tamilnadu, India.\\nPhone No.  :  044 4627 7555\\nEmail Address : ugadmission@vit.ac.in\\nWebsite : www.vit.ac.in \\nVIT KOTA Information Centre\\n117, Near Nucleus Coaching,\\nOld Rajiv Gandhi Nagar,\\nKota (Rajasthan) 324005\\nContact No. : 9782376476, 7690074787\\nEmail: aadilnaki.pathan@vit.ac.in\\nWebsite : www.vit.ac.in\\nAccreditation & Ranking\\nPlacement (2023 Graduated Batch)\\nFor More Details\\n16\\nLegal Jurisdiction : All suits and actions arising out of or relating to VIT shall be instituted within the Jurisdiction of courts at Vellore, Tamilnadu only.\\n947\\nNo. of\\nCompanies\\n4,493\\nNo. of Super\\nDream Oﬀers\\n3,540\\n14,565\\nNo. of\\nDream Oﬀers\\n6,532\\nNo. of\\nRegular Oﬀers\\nOverall\\nOﬀers\\n1.02 Cr.\\nHighest\\nPackage (Per Annum)\\nIncludes Internships & Placements (10LPA & Above)\\nAccredited with the highest\\ngrade A++ by NAAC\\nRanked #11\\nBest Engineering Institution\\nof India\\nRanked #163 in Asia by\\nQS World University\\nRanking - Asia, 2024.')]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "retriever.invoke(\"VIT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "MZvtcrf7CGbQ",
        "outputId": "fc18f600-c01b-41de-ead8-d4367129fb15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 14, 'page_label': '15'}, page_content='B.TECH.- Bioengineering\\nB.TECH.- Biotechnology\\nB.TECH.- Chemical Engineering\\nB.TECH.- Civil Engineering\\nB.TECH.- Electrical and Electronics Engineering\\nB.TECH.- Electronics and Instrumentation Engineering\\nB.TECH.- Fashion Technology\\nB.TECH.- Aerospace Engineering\\nB.TECH.- Computer Science & Engineering (E-Commerce Technology)\\nB.TECH.- Computer Science & Engineering (Education Technology)\\nB.TECH.- Computer Science and Engineering and Business Systems\\nB.TECH.- Computer Science and Engineering\\nB.TECH.- Computer Science and Engineering (Artiﬁcial Intelligence and Machine Learning)\\nB.TECH.- Computer Science and Engineering (Artiﬁcial Intelligence and Robotics)\\nB.TECH.- Computer Science and Engineering (Bioinformatics)\\nB.TECH.- Computer Science and Engineering (Block Chain Technology)\\nB.TECH.- Computer Science and Engineering (Block Chain)\\nB.TECH.- Computer Science and Engineering (Cyber Physical Systems)\\nB.TECH.- Computer Science and Engineering (Cyber Security)\\nB.TECH.- Computer Science and Engineering (Data Science)\\nB.TECH.- Computer Science and Engineering (Data Analytics)\\nB.TECH.- Computer Science and Engineering (Information Security)\\nB.TECH.- Computer Science and Engineering (Internet of Things)\\nB.TECH.- Computer Science & Engineering (Cloud Computing & Automation)\\nB.TECH.- Computer Science & Engineering (Health Informatics)\\nB.TECH.- Computer Science and Engineering and Business Systems (in collaboration with TCS)\\nB.TECH.- Computer Science & Engineering (Cyber Security & Digital Forensics)\\nB.TECH.- Computer Science & Engineering (Gaming Technology)\\nB.TECH.- Computer Science and Engineering (Software Engineering)\\nB.TECH.- Electrical and Computer Science Engineering\\nB.TECH.- Electronics and Communication Engineering\\nB.TECH.- Electronics and Communication Engineering (Biomedical Engineering)\\nB.TECH.- Electronics & Communication Engineering (Artiﬁcial Intelligence & Cybernetics)\\nB.TECH.- Electronics and Communication Engineering (Embedded Systems)\\nB.TECH.- Electronics and Communication Engineering (VLSI)\\nB.TECH.- Electronics and Computer Engineering\\nB.TECH.- Electronics Engineering (VLSI Design and Technology)\\nB.TECH.- Information Technology\\nB.TECH.- Mechanical Engineering\\nB.TECH.- Mechanical Engineering (Automotive Design)\\nB.TECH.- Mechanical Engineering (Electric Vehicles)\\nB.TECH.- Mechanical Engineering (Manufacturing Engineering)\\nB.TECH.- Mechanical Engineering (Artiﬁcial Intelligence & Robotics)\\nB.TECH.- Mechanical Engineering (Robotics)\\nB.TECH.- Mechatronics and Automation\\n(*after concession)\\nGroup - B\\nGroup - A\\nFee Stucture :\\nTuition Fees (Per Annum)\\nCaution Deposit (Refundable) (One time payment)\\nTotal Fees to be paid for the ﬁrst year\\nINR 1,95,000*\\nINR 3,000\\nINR 1,98,000\\n(*after concession)\\nFee Stucture :\\nTuition Fees (Per Annum)\\nCaution Deposit (Refundable) (One time payment)\\nTotal Fees to be paid for the ﬁrst year\\nINR 1,73,000*\\nINR 3,000\\nINR 1,76,000\\n12. Fee Structure\\n12'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 4, 'page_label': '5'}, page_content='Vellore Chennai AP Bhopal\\nB.Tech - Computer Science & Engineering\\n(Gaming Technology)\\nVellore Chennai AP BhopalVellore Chennai AP Bhopal\\nVellore Chennai AP BhopalVellore Chennai AP Bhopal\\nVellore Chennai AP BhopalVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nB.Tech - Electronics and Instrumentation\\nEngineering\\nB.Tech - Electronics and Computer EngineeringB.Tech - Electrical and Computer Science\\nEngineering\\nB.Tech - Fashion Technology\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP BhopalVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nB.Tech - Computer Science & Engineering\\n(E-Commerce Technology)\\nB.Tech - Electronics & Communication\\nEngineering (Artiﬁcial Intelligence & Cybernetics)\\nB.Tech - Electronics Engineering\\n(VLSI Design and Technology)\\nB.Tech - Electrical and Electronics Engineering\\nB.Tech - Electronics & Communication\\nEngineering\\nB.Tech - Electronics and Communication\\nEngineering (Biomedical Engineering)\\nB.Tech - Mechanical Engineering\\n(Artiﬁcial Intelligence & Robotics)\\nB.Tech - Information Technology\\nB.Tech - Computer Science and Engineering\\n(Cyber Security)\\nB.Tech - Mechanical Engineering\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nB.Tech - Mechatronics and Automation\\nB.Tech - Computer Science & Engineering\\n(Education Technology)\\nB.Tech - Mechanical(Manufacturing Engineering)\\n19 20\\n21\\nVellore Chennai AP Bhopal\\nB.Tech - Computer Science and Engineering\\n(Blockchain)\\n23\\n28\\nVellore Chennai AP Bhopal\\nB.Tech - Electronics and Communication\\nEngineering (Embedded Systems)\\n30\\nVellore Chennai AP Bhopal\\nB.Tech - Electronics and Communication\\nEngineering (VLSI)\\n31\\n27\\n32\\n29\\n3433\\n3635\\n3837\\n39\\n24\\nVellore Chennai AP Bhopal\\nB.Tech - Computer Science and Engineering\\n(Software Engineering)\\nVellore Chennai AP Bhopal\\nB.Tech - Computer Science and Engineering\\n(Data Analytics)\\n25 26\\n40\\n45\\n43\\nVellore Chennai AP Bhopal\\nB.Tech - Mechanical Engineering\\n(Electric Vehicles)\\nVellore Chennai AP Bhopal\\nB.Tech - Mechanical Engineering\\n(Automotive Design)\\n44\\n41\\nVellore Chennai AP Bhopal\\nB.Tech - Mechanical Engineering\\n(Robotics)\\n42\\n2\\nVisit https://vitap.ac.in/btech/ for full list of specialisation / Minor / Double Major options\\nVellore Chennai AP Bhopal\\nB.Tech - Computer Science & Engineering\\n(Artiﬁcial Intelligence & Machine Learning)\\n22'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 5, 'page_label': '6'}, page_content='3.1. Guidelines for ﬁlling up the application form\\nThe B.Tech Admission Application Form is available online at www.viteee.vit.ac.in. The Cost of the Application Form is Rs. 1,350 /- \\n(Non-Refundable).\\nValid e-mail address and active mobile number are required.\\nEnter all the relevant details carefully.\\nAn OTP will be sent to your Registered Indian Mobile Number for veriﬁcation.\\nOnce OTP is validated, a password will be sent to your registered email address.\\nAll the correspondence shall be made to the registered email address and mobile number \\nonly.\\n3.1.1. New User Registration\\nValid e-mail address and active mobile number are required.\\n3.1.2. Sign-in for Registered Users\\nFill in the relevant ﬁelds carefully. An Application Number will be generated after ﬁlling \\nprimary details.\\nRefer this application number in all your future correspondence.\\n3.1.3. Filling online application form\\nApplication cost of Rs.1350 should be paid through Net Banking Account/ Credit/ \\nDebit Card/ Paytm\\nFor Abroad test centre : Equivalent to USD 90 in INR\\nNote : Application cost is non-refundable\\n3.1.4. Payment\\nStep\\n04\\nStep\\n03\\nStep\\n02\\nStep\\n01\\n3. Online Application Form\\n3'),\n",
              " Document(metadata={'producer': 'Adobe PDF library 17.00', 'creator': 'Adobe Illustrator 27.9 (Windows)', 'creationdate': '2023-12-12T12:45:23+06:30', 'moddate': '2023-12-13T10:15:04+05:30', 'title': 'VITEEE BROCHURE_2024_Updated 11', 'source': 'VITEEE-2024-information-brochure.pdf', 'total_pages': 20, 'page': 3, 'page_label': '4'}, page_content='VIT (Vellore Institute of Technology), established in 1984, was conferred with the status of a deemed university in 2001 by the Govern-\\nment of India. In three decades, the Institution has emerged as an innovator in higher education, growing from 180 students to 67,000+ \\nstudents. Today, it is counted among the most prominent and respected universities in India, featuring top-ranked academic \\nprogrammes. The main campus of VIT is at Vellore. It started another campus in Chennai subsequently VIT - AP at Amaravati and VIT - \\nBhopal at Madhya Pradesh were started as State Private Universities later. Eco-friendly in design and equipped with cutting-edge \\ninfrastructure, VIT provides a favorable atmosphere for learning and living. Quality in teaching-learning, research, and innovation are at \\nthe core of education at VIT. The institution follows a student-centered culture and places the highest importance on quality and consis -\\ntency in learning, teaching, and research experiences.\\nAbout VIT1.\\nB.Tech - Aerospace Engineering\\nVellore Chennai AP Bhopal\\nB.Tech - Biotechnology\\nVellore Chennai AP Bhopal\\nB.Tech - Bioengineering\\nVellore Chennai AP Bhopal\\nB.Tech - Chemical Engineering\\nVellore Chennai AP Bhopal\\nB.Tech - Civil Engineering\\nVellore Chennai AP Bhopal Vellore Chennai AP Bhopal\\nVellore Chennai AP BhopalVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nVellore Chennai AP Bhopal\\nB.Tech - Computer Science and Engineering\\n(Artiﬁcial Intelligence and Robotics)\\nB.Tech - Computer Science & Engineering\\n(Cyber Security & Digital Forensics)\\nB.Tech - Computer Science and Engineering\\n(Health Informatics)\\nB.Tech - Computer Science & Engineering\\n(Cloud Computing & Automation)\\nB.Tech - Computer Science and Engineering\\n(Bioinformatics)\\nB.Tech - Computer Science and Engineering and\\nBusiness systems (in collaboration with TCS)\\nB.Tech - Computer Science and Engineering\\n(Information Security)\\nB.Tech - Computer Science and Engineering\\nand Business Systems\\nB.Tech - Computer Science and Engineering\\n(Data Science)\\nB.Tech - Computer Science and Engineering\\n(Internet of Things)\\nB.Tech - Computer Science and Engineering\\n(Cyber Physical Systems)\\nB.Tech - Computer Science and Engineering\\nB.Tech - Computer Science and Engineering\\n(Block Chain Technology)\\n01\\n03\\n05\\n07\\n10\\n12\\n14\\n16\\n02\\n04\\n06\\n08\\n09\\n11\\n13\\n15\\n17 18\\n2. B.Tech. Programmes Oﬀered\\n1')]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "retriever.invoke(\"B.Tech\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "tQcxsGFrCGbQ"
      },
      "outputs": [],
      "source": [
        "chain = (\n",
        "    {\n",
        "        \"context\":itemgetter(\"question\") | retriever,\n",
        "        \"question\":itemgetter(\"question\")\n",
        "    }\n",
        "    | prompt\n",
        "    | model\n",
        "    | parser\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "W7SbBYqnCGbQ",
        "outputId": "368366a9-388d-4991-c4ed-7f146cecb283",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I don't know the answer to this question based on the provided context. The context appears to be related to the VITEEE brochure and admission process, and does not mention LLMs (Large Language Models) or provide any relevant information for an essay on the topic.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "chain.invoke({\"question\":\"Give an essay on LLMs\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "c8cUIB_cCGbQ",
        "outputId": "fe89f012-4236-49e9-9c1f-d1f2b020d374",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The VITEEE 2024 exam is tentatively scheduled to be conducted between April 19 and 30, 2024. The number of days will vary for test cities.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "chain.invoke({\"question\":\"When is the exam?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "DkMtmYjLCGbQ",
        "outputId": "26269e6a-5ca7-452a-b8be-54bd9c10d9a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The context provided does not explicitly list all the courses offered by VIT. However, it mentions \"B. Tech Programmes Oﬀered\" and \"admission to undergraduate engineering programmes\" which suggests that VIT offers various B.Tech programs. The exact courses are not specified in the given context.\n"
          ]
        }
      ],
      "source": [
        "print(chain.invoke({\"question\":\"What are the courses offered by VIT?\"}))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dIXPgs12PKGa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "169d32e1e2374e30b3f49aea5c31eb5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c56b471d5db84145965f23d71965d905",
              "IPY_MODEL_a32870fe2cef4de4b451a108d3312c31",
              "IPY_MODEL_25879ef600e146058f1ba006ca319510"
            ],
            "layout": "IPY_MODEL_52d696cf397e448b892e1ec47bde5d7e"
          }
        },
        "c56b471d5db84145965f23d71965d905": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_671ac300d02b4843bcefa54387cd52be",
            "placeholder": "​",
            "style": "IPY_MODEL_f2720aaa4cc54a0da94650ec0af53cf5",
            "value": "modules.json: 100%"
          }
        },
        "a32870fe2cef4de4b451a108d3312c31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1689669297cd4d5fb333e5c9bd178c06",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5890bc1b6f524a7792962b2a95cd88ff",
            "value": 349
          }
        },
        "25879ef600e146058f1ba006ca319510": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec64225aaa54457398e99e50b97959bf",
            "placeholder": "​",
            "style": "IPY_MODEL_d26534653f3e463b928ad88605de6bf6",
            "value": " 349/349 [00:00&lt;00:00, 19.7kB/s]"
          }
        },
        "52d696cf397e448b892e1ec47bde5d7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "671ac300d02b4843bcefa54387cd52be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2720aaa4cc54a0da94650ec0af53cf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1689669297cd4d5fb333e5c9bd178c06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5890bc1b6f524a7792962b2a95cd88ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec64225aaa54457398e99e50b97959bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d26534653f3e463b928ad88605de6bf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbef60af59ca41a397fa9f0d1fb3e345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2c804de574c4b189c27e99acb33637e",
              "IPY_MODEL_d42b8789def54eb6beb49f68c3d20ce6",
              "IPY_MODEL_5bb71cc7374143828ca5d9945b8b4a5a"
            ],
            "layout": "IPY_MODEL_8cd75f9da49348b78640299e78ae410b"
          }
        },
        "c2c804de574c4b189c27e99acb33637e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a225e4556e83413487ea17b753497013",
            "placeholder": "​",
            "style": "IPY_MODEL_d1b99d1e209142ac842cf420d2c98967",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "d42b8789def54eb6beb49f68c3d20ce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ccdf81da6e24339989b58d5011e7d60",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0a4b9360b0a42728b5078000461f249",
            "value": 124
          }
        },
        "5bb71cc7374143828ca5d9945b8b4a5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0d2bf6a8f374edface751cfe8c4cbba",
            "placeholder": "​",
            "style": "IPY_MODEL_8cc5b2294b2b471c993829b6f3e0d33b",
            "value": " 124/124 [00:00&lt;00:00, 4.77kB/s]"
          }
        },
        "8cd75f9da49348b78640299e78ae410b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a225e4556e83413487ea17b753497013": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1b99d1e209142ac842cf420d2c98967": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ccdf81da6e24339989b58d5011e7d60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0a4b9360b0a42728b5078000461f249": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0d2bf6a8f374edface751cfe8c4cbba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cc5b2294b2b471c993829b6f3e0d33b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09d6d9002a9644f0acad1ea521b191d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e32cf93fa1d4c319dc6dc8239980e11",
              "IPY_MODEL_a3e761d09e44453a94740cef92746ebd",
              "IPY_MODEL_d6071e83cf1c47909811fe7ad718eadf"
            ],
            "layout": "IPY_MODEL_d3eade9599004ec9ac018fe5a4f2aef5"
          }
        },
        "1e32cf93fa1d4c319dc6dc8239980e11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c8ff67f019b40e6843c974473453bc3",
            "placeholder": "​",
            "style": "IPY_MODEL_ae786c582fe545c4956284c27edb23fb",
            "value": "README.md: 100%"
          }
        },
        "a3e761d09e44453a94740cef92746ebd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76cb181b205a415b88ee94bebfcba0e9",
            "max": 94783,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a23c75f2b42d42e3b95ce12fe72d740a",
            "value": 94783
          }
        },
        "d6071e83cf1c47909811fe7ad718eadf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bc6960a25224b99afeea0362548fe0c",
            "placeholder": "​",
            "style": "IPY_MODEL_e042fa9021ea4de8acc3c4c368477985",
            "value": " 94.8k/94.8k [00:00&lt;00:00, 2.54MB/s]"
          }
        },
        "d3eade9599004ec9ac018fe5a4f2aef5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c8ff67f019b40e6843c974473453bc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae786c582fe545c4956284c27edb23fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76cb181b205a415b88ee94bebfcba0e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a23c75f2b42d42e3b95ce12fe72d740a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0bc6960a25224b99afeea0362548fe0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e042fa9021ea4de8acc3c4c368477985": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77d86724afa24245888d9b090024107e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_80488676dcbb494bbcf4faa239a12681",
              "IPY_MODEL_3fa6e4f704ad40c7ba27030b50a6c62e",
              "IPY_MODEL_b37f285664d741dca97c655dc40d489f"
            ],
            "layout": "IPY_MODEL_13c4a66546e6459d8593981f9a3e4f30"
          }
        },
        "80488676dcbb494bbcf4faa239a12681": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f3cbce7d987415aa29fb282e7976762",
            "placeholder": "​",
            "style": "IPY_MODEL_67fbd275cb5c47e8a669e6d6d54ee449",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "3fa6e4f704ad40c7ba27030b50a6c62e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c3ab34e49f4487ab5595461e5f8f48a",
            "max": 52,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e07c1a6edaf3438fa4798e315e86a782",
            "value": 52
          }
        },
        "b37f285664d741dca97c655dc40d489f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5413c2c459c2445ca31cff4631c27d35",
            "placeholder": "​",
            "style": "IPY_MODEL_b405e20920b24f48b4c831e8e6987687",
            "value": " 52.0/52.0 [00:00&lt;00:00, 3.56kB/s]"
          }
        },
        "13c4a66546e6459d8593981f9a3e4f30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f3cbce7d987415aa29fb282e7976762": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67fbd275cb5c47e8a669e6d6d54ee449": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c3ab34e49f4487ab5595461e5f8f48a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e07c1a6edaf3438fa4798e315e86a782": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5413c2c459c2445ca31cff4631c27d35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b405e20920b24f48b4c831e8e6987687": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8b95fadac574f03a7a36c88cb1085e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da75015aecd044d7b4035077ada6c05b",
              "IPY_MODEL_fe6f174458ac49dabd2fc81def7869af",
              "IPY_MODEL_b6b5a1f798a44ea7b2ffcffdb5206b1f"
            ],
            "layout": "IPY_MODEL_ae7139f8ca254de6ad3e3e1089b3e14e"
          }
        },
        "da75015aecd044d7b4035077ada6c05b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4ffe32b2b7f47da9c72c2d3084d90e9",
            "placeholder": "​",
            "style": "IPY_MODEL_d1b821f8c68644cd8efd241273cf5514",
            "value": "config.json: 100%"
          }
        },
        "fe6f174458ac49dabd2fc81def7869af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fda6d4dc40ce404c9b7c0dff593c7619",
            "max": 743,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b30606f6ac4841d49d712429f4cd7351",
            "value": 743
          }
        },
        "b6b5a1f798a44ea7b2ffcffdb5206b1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdbf443c167a4bd280723c1f008e1947",
            "placeholder": "​",
            "style": "IPY_MODEL_164c54c1c903456cad3fe9dbfde52d2c",
            "value": " 743/743 [00:00&lt;00:00, 42.1kB/s]"
          }
        },
        "ae7139f8ca254de6ad3e3e1089b3e14e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4ffe32b2b7f47da9c72c2d3084d90e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1b821f8c68644cd8efd241273cf5514": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fda6d4dc40ce404c9b7c0dff593c7619": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b30606f6ac4841d49d712429f4cd7351": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cdbf443c167a4bd280723c1f008e1947": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "164c54c1c903456cad3fe9dbfde52d2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d5edc94566c43ebaf82a2845f064584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64253b8addd4427286f3dbc6f6bf1f2e",
              "IPY_MODEL_48966039760445a4a22bd680f90816f4",
              "IPY_MODEL_f42e5e9dde4b44439db29e42586e4536"
            ],
            "layout": "IPY_MODEL_dbd2aac6352941b38d02f7e9e15447e5"
          }
        },
        "64253b8addd4427286f3dbc6f6bf1f2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79d1192e29764020bf2048976b895c83",
            "placeholder": "​",
            "style": "IPY_MODEL_fbf687b576c545128d1089f4c2fd3f3b",
            "value": "model.safetensors: 100%"
          }
        },
        "48966039760445a4a22bd680f90816f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b6c6406e2fc47d38d74ed38e0e14caa",
            "max": 133466304,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6adccbcebe584d28ab03211f78504d02",
            "value": 133466304
          }
        },
        "f42e5e9dde4b44439db29e42586e4536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e949b65cd7c4b72bf60035cc2a550f8",
            "placeholder": "​",
            "style": "IPY_MODEL_2cff5472964845d0ade105c01db8cd3b",
            "value": " 133M/133M [00:01&lt;00:00, 149MB/s]"
          }
        },
        "dbd2aac6352941b38d02f7e9e15447e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79d1192e29764020bf2048976b895c83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbf687b576c545128d1089f4c2fd3f3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b6c6406e2fc47d38d74ed38e0e14caa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6adccbcebe584d28ab03211f78504d02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e949b65cd7c4b72bf60035cc2a550f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cff5472964845d0ade105c01db8cd3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab2421b2b0944792a3a0c44f87070962": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d13799042f94a5ea537c6fcb1cb2df9",
              "IPY_MODEL_b3061a5ecabb425d85e789a91baa40a9",
              "IPY_MODEL_e7fd88ef2b364b3c9d992bfc710312b4"
            ],
            "layout": "IPY_MODEL_e46274912a934daaa72c6282f5bbee2d"
          }
        },
        "9d13799042f94a5ea537c6fcb1cb2df9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfd8dd90e5d94c90bbe4db03fd0ecba9",
            "placeholder": "​",
            "style": "IPY_MODEL_f88b6de8d0514ba9bfa0710d66a79448",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "b3061a5ecabb425d85e789a91baa40a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79557332e85646d8a83ec2c250e7e7e2",
            "max": 366,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d0c62013505413b9b08ea96911ebf98",
            "value": 366
          }
        },
        "e7fd88ef2b364b3c9d992bfc710312b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df83648ec4c34690885457d20caff082",
            "placeholder": "​",
            "style": "IPY_MODEL_2fcbffafda6b42f19d110d631cdbe46b",
            "value": " 366/366 [00:00&lt;00:00, 18.8kB/s]"
          }
        },
        "e46274912a934daaa72c6282f5bbee2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfd8dd90e5d94c90bbe4db03fd0ecba9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f88b6de8d0514ba9bfa0710d66a79448": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79557332e85646d8a83ec2c250e7e7e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d0c62013505413b9b08ea96911ebf98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df83648ec4c34690885457d20caff082": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fcbffafda6b42f19d110d631cdbe46b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f4addd80f054b20a71082231c7935c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa77b89197924e8885372ed1eecd2d6e",
              "IPY_MODEL_00cd01bd7d954784bc8e4e3b03b0ec27",
              "IPY_MODEL_ae86b2e9f0f241bcb992cf4ce1d5acd6"
            ],
            "layout": "IPY_MODEL_db7b5d971e4e4585adc7242d9ba48f0c"
          }
        },
        "aa77b89197924e8885372ed1eecd2d6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a7b777c975a41e5b4a835281c107321",
            "placeholder": "​",
            "style": "IPY_MODEL_da8269fbd7d24eebade7ad05904ddb44",
            "value": "vocab.txt: 100%"
          }
        },
        "00cd01bd7d954784bc8e4e3b03b0ec27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61a7ee33581548db8a5081bea39ccbee",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5490d51f6b0e43ca9660d8965167917b",
            "value": 231508
          }
        },
        "ae86b2e9f0f241bcb992cf4ce1d5acd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce434bbc29004427952c1d8c493ed132",
            "placeholder": "​",
            "style": "IPY_MODEL_bcd0cd5d0de3406780c4128fbfef1126",
            "value": " 232k/232k [00:00&lt;00:00, 3.63MB/s]"
          }
        },
        "db7b5d971e4e4585adc7242d9ba48f0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a7b777c975a41e5b4a835281c107321": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da8269fbd7d24eebade7ad05904ddb44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61a7ee33581548db8a5081bea39ccbee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5490d51f6b0e43ca9660d8965167917b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce434bbc29004427952c1d8c493ed132": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcd0cd5d0de3406780c4128fbfef1126": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "058b7b53e3024257b281efff8a0c02ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0b534e87e054c4ab234bdffbade062a",
              "IPY_MODEL_6ed8868054b344f8a75baf107f2cbbcc",
              "IPY_MODEL_e38d66c3df0e4beea8502de2cf326274"
            ],
            "layout": "IPY_MODEL_4137b248354c48cb8382c0c02f4e4e8b"
          }
        },
        "e0b534e87e054c4ab234bdffbade062a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a87ec6b77bb840689e77ce5b94af6adc",
            "placeholder": "​",
            "style": "IPY_MODEL_46cd03ce161f40d4bcebe1ddad7683f4",
            "value": "tokenizer.json: 100%"
          }
        },
        "6ed8868054b344f8a75baf107f2cbbcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c7b64f111654130905828818ab3da34",
            "max": 711396,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d70c1c84f8e041c39e08b41c21fbeb95",
            "value": 711396
          }
        },
        "e38d66c3df0e4beea8502de2cf326274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16f556540399443bbefe032aae7de0ab",
            "placeholder": "​",
            "style": "IPY_MODEL_54f15c75510a4e33b9d133b8d99e3a91",
            "value": " 711k/711k [00:00&lt;00:00, 13.8MB/s]"
          }
        },
        "4137b248354c48cb8382c0c02f4e4e8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a87ec6b77bb840689e77ce5b94af6adc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46cd03ce161f40d4bcebe1ddad7683f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c7b64f111654130905828818ab3da34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d70c1c84f8e041c39e08b41c21fbeb95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16f556540399443bbefe032aae7de0ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54f15c75510a4e33b9d133b8d99e3a91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b35a1209807640e284041918d2b60252": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7fcda9face474f85b3980c06cb9297a0",
              "IPY_MODEL_7c0117876eb34b8a95db214ca9ca7311",
              "IPY_MODEL_3a554b41f2e24136b6fac8112de264af"
            ],
            "layout": "IPY_MODEL_f8e9b55f7d11457199d57cadf699ce3a"
          }
        },
        "7fcda9face474f85b3980c06cb9297a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3f63b2ede1f407893e9aa50a5b7eeeb",
            "placeholder": "​",
            "style": "IPY_MODEL_fff9d760ec284cbaa537ae39614d3756",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "7c0117876eb34b8a95db214ca9ca7311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd1b5ea1e07248eebfdbc68118bc00f3",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d57bc8b1bdcb49719ee4a31b3cb680cd",
            "value": 125
          }
        },
        "3a554b41f2e24136b6fac8112de264af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70bcd78ff42f4b7ab1899b0d77ff64f5",
            "placeholder": "​",
            "style": "IPY_MODEL_ddeb8aaf0866467493d9de8fe7e69bc5",
            "value": " 125/125 [00:00&lt;00:00, 7.15kB/s]"
          }
        },
        "f8e9b55f7d11457199d57cadf699ce3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3f63b2ede1f407893e9aa50a5b7eeeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fff9d760ec284cbaa537ae39614d3756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd1b5ea1e07248eebfdbc68118bc00f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d57bc8b1bdcb49719ee4a31b3cb680cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "70bcd78ff42f4b7ab1899b0d77ff64f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddeb8aaf0866467493d9de8fe7e69bc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e085fe2b4b7740a88380bea6e56de746": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ba8e37b335f4de8a638b98aa991d15a",
              "IPY_MODEL_a606f86d95e84aeeb8c7144d8505ca8b",
              "IPY_MODEL_e04120a768504259b3d540c91e69f3cb"
            ],
            "layout": "IPY_MODEL_d9580191c17a459fbdd04c9ceb7d7a9d"
          }
        },
        "8ba8e37b335f4de8a638b98aa991d15a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_acbb5c59b0b54085917286cb9fee5a37",
            "placeholder": "​",
            "style": "IPY_MODEL_a0be87b6b58c41898713304faaf08f8c",
            "value": "config.json: 100%"
          }
        },
        "a606f86d95e84aeeb8c7144d8505ca8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_537a078aef4b46eaa78b9c3fd2fe1960",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ea679c764cf4df3828677019e980212",
            "value": 190
          }
        },
        "e04120a768504259b3d540c91e69f3cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_374c5b48b7724910b886fab2a699abff",
            "placeholder": "​",
            "style": "IPY_MODEL_35a2c334dcbc4605816e65f588cc823a",
            "value": " 190/190 [00:00&lt;00:00, 7.93kB/s]"
          }
        },
        "d9580191c17a459fbdd04c9ceb7d7a9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acbb5c59b0b54085917286cb9fee5a37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0be87b6b58c41898713304faaf08f8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "537a078aef4b46eaa78b9c3fd2fe1960": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ea679c764cf4df3828677019e980212": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "374c5b48b7724910b886fab2a699abff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35a2c334dcbc4605816e65f588cc823a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}