{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Advanced RAG for Question Generation\n",
    "\n",
    "This notebook implements an agentic system for generating questions from a document. It leverages advanced RAG techniques and an autonomous agent to automate and refine the question generation process.\n",
    "\n",
    "The agent's workflow consists of four key features:\n",
    "1.  **Automated Query Generation**: Automatically creates initial questions/topics from the document.\n",
    "2.  **Iterative Refinement**: Employs a generate-critique-improve loop to enhance question quality.\n",
    "3.  **Self-Correction/Validation**: Checks generated questions against the source document to ensure they are answerable and accurate.\n",
    "4.  **Dynamic Tool Use**: Can independently decide to use external tools, like a web search, to gather more context and enrich the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, we install and import the necessary libraries. We'll need libraries for document loading, splitting, embeddings, vector storage, and the LangChain framework to build our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain langchain-community langchain-huggingface torch transformers faiss-cpu sentence-transformers beautifulsoup4 duckduckgo-search\n",
    "!pip install -qU pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from getpass import getpass\n",
    "\n",
    "# LangChain components\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceInstructEmbeddings, HuggingFaceEndpoint\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import AgentExecutor, create_react_agent, Tool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Core RAG Pipeline\n",
    "\n",
    "Here, we set up the core components of our advanced RAG system, based on the techniques in your `rag.ipynb` notebook. This includes setting up the embedding model, the LLM endpoint, and loading the source document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Securely get Hugging Face API token\n",
    "if not (hf_token := os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")):\n",
    "    hf_token = getpass(\"Enter your Hugging Face API token: \")\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_token\n",
    "\n",
    "# Download the source PDF document (e.g., \"Attention Is All You Need\")\n",
    "pdf_url = \"https://arxiv.org/pdf/1706.03762.pdf\"\n",
    "pdf_path = \"attention_is_all_you_need.pdf\"\n",
    "if not os.path.exists(pdf_path):\n",
    "    response = requests.get(pdf_url)\n",
    "    with open(pdf_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "print(f\"Document downloaded to {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the document\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Document split into {len(docs)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the embedding model (Instructor-XL)\n",
    "embedding_model = HuggingFaceInstructEmbeddings(\n",
    "    model_name=\"hkunlp/instructor-xl\",\n",
    "    model_kwargs={\"device\": \"cpu\"} # Use 'cuda' if GPU is available\n",
    ")\n",
    "\n",
    "# Create the FAISS vector store\n",
    "vector_db = FAISS.from_documents(docs, embedding_model)\n",
    "\n",
    "# Create the retriever\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "print(\"Vector store and retriever created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LLM (Flan-T5-XXL)\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"google/flan-t5-xxl\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=300,\n",
    "    top_k=30,\n",
    "    huggingfacehub_api_token=hf_token\n",
    ")\n",
    "\n",
    "# Create the base RetrievalQA chain for answering questions\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"LLM and QA chain are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement the Agentic Workflow\n",
    "\n",
    "Now, we build the agent that automates the question generation process. This involves:\n",
    "1.  **Defining Tools**: A tool for answering questions using our RAG pipeline and a web search tool.\n",
    "2.  **Creating the Agent**: Designing a prompt that instructs the agent to follow the generate-critique-refine loop.\n",
    "3.  **Running the Agent Executor**: Kicking off the autonomous process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Tools for the Agent\n",
    "\n",
    "# Tool to use the RAG pipeline for answering/validation\n",
    "rag_tool = Tool(\n",
    "    name=\"document_qa_system\",\n",
    "    func=lambda q: qa_chain({\"query\": q}),\n",
    "    description=\"Use this to answer questions or validate information against the source document. Input is the question to be answered.\"\n",
    ")\n",
    "\n",
    "# Tool for web search\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "tools = [rag_tool, search_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create the Agent\n",
    "\n",
    "# The core prompt that drives the agent's behavior\n",
    "agent_prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert agent designed to generate high-quality, insightful questions from a source document.\n",
    "Your goal is to autonomously explore a topic within the document and formulate a question that is clear, relevant, and answerable by the document's content.\n",
    "\n",
    "You must follow this iterative process:\n",
    "1.  **Generate**: Start with an initial question based on the given topic.\n",
    "2.  **Critique**: Analyze the question. Is it clear? Is it specific? Can it likely be answered by the `document_qa_system`? Does it require external knowledge? If the document context seems insufficient for a deep question, decide if a web search could provide useful context (e.g., for definitions or related concepts).\n",
    "3.  **Refine**: If the critique reveals weaknesses OR if you used the web search to find more context, rewrite the question to improve it. A good refined question is often more specific and nuanced.\n",
    "4.  **Validate**: Use the `document_qa_system` with your refined question. If the system provides a confident, well-supported answer, the question is valid. If the answer is poor or irrelevant, you may need to refine the question again.\n",
    "\n",
    "TOOLS:\n",
    "------\n",
    "You have access to the following tools: {tools}\n",
    "\n",
    "To use a tool, please use the following format:\n",
    "```\n",
    "Thought: Do I need to use a tool? Yes\n",
    "Action: The action to take, should be one of [{tool_names}]\n",
    "Action Input: The input to the action\n",
    "Observation: The result of the action\n",
    "```\n",
    "\n",
    "When you have a final, validated question, you MUST output it in the following format:\n",
    "```\n",
    "Thought: I have a final, validated question.\n",
    "Final Answer: [Your final, high-quality question here]\n",
    "```\n",
    "\n",
    "Begin!\n",
    "\n",
    "Topic to explore: {input}\n",
    "Thought Process Log: {agent_scratchpad}\n",
    "\"\"\")\n",
    "\n",
    "agent = create_react_agent(llm, tools, agent_prompt_template)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run the Agent and Generate Questions\n",
    "\n",
    "Now, we'll kickstart the process. First, we'll do a simple LLM call to generate a few high-level topics from the document. Then, we'll feed these topics to our agent one by one to generate refined, validated questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Automatically generate initial topics from the document\n",
    "topic_generation_prompt = PromptTemplate.from_template(\n",
    "    \"Based on the first few pages of the provided document, list 3-5 high-level topics or concepts that could be used to generate detailed questions. \\\n",
    "     Focus on the core ideas presented. Return the topics as a numbered list.\\\n",
    "     Document context: {context}\"\n",
    ")\n",
    "\n",
    "initial_context = \"\\n\\n\".join([doc.page_content for doc in docs[:3]]) # Use first 3 chunks\n",
    "\n",
    "topic_generation_chain = topic_generation_prompt | llm\n",
    "generated_topics_str = topic_generation_chain.invoke({\"context\": initial_context})\n",
    "\n",
    "topics = [line.strip() for line in generated_topics_str.split('\n') if line.strip() and line[0].isdigit()]\n",
    "\n",
    "print(\"Automatically Generated Topics:\")\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Run the agent on each topic to generate a refined question\n",
    "final_questions = []\n",
    "for topic in topics:\n",
    "    print(f\"--- Running Agent for Topic: '{topic}' ---\")\n",
    "    try:\n",
    "        result = agent_executor.invoke({\"input\": topic})\n",
    "        final_questions.append(result['output'])\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing topic '{topic}': {e}\")\n",
    "        final_questions.append(f\"Failed to generate question for topic: {topic}\")\n",
    "    print(\"---\")\n",
    "\n",
    "print(\"\\n=== Final Generated Questions ===\")\n",
    "for i, q in enumerate(final_questions):\n",
    "    print(f\"{i+1}. {q}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}