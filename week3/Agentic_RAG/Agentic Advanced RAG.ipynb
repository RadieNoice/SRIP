{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Advanced RAG for Question Generation\n",
    "\n",
    "This notebook converts the advanced RAG pipeline into an agentic system. The core idea is to wrap the entire RAG process (retrieval, reranking, and generation) into a single tool that a LangChain agent can decide to use. This allows for more complex and multi-step reasoning in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv sentence-transformers langchain langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import json\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.agents import Tool, AgentExecutor, create_react_agent\n",
    "from langchain import hub\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --- Configuration ---\n",
    "googleapikey = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "vector_db_path = r\"../vector_db\"  # Adjusted path\n",
    "model = \"gemini-1.5-flash\"\n",
    "temp = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the RAG Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_function():\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "        encode_kwargs={\"batch_size\": 4, \"normalize_embeddings\": True}\n",
    "    )\n",
    "\n",
    "reranker = CrossEncoder(\"BAAI/bge-reranker-v2-m3\", max_length=512)\n",
    "\n",
    "def parse_questions(text: str, total_questions: int, path: str = None):\n",
    "    # This function remains the same as in your original notebook\n",
    "    pass\n",
    "\n",
    "def query_rag_tool(input_str: str):\n",
    "    try:\n",
    "        query_text, no_of_questions_str, output_path, subject, chapter_str = input_str.split('|')\n",
    "        no_of_questions = int(no_of_questions_str)\n",
    "        chapter = int(chapter_str)\n",
    "    except ValueError:\n",
    "        return \"Invalid input format. Expected: query_text|no_of_questions|output_path|subject|chapter\"\n",
    "\n",
    "    embedding_function = get_embedding_function()\n",
    "    db = Chroma(persist_directory=vector_db_path, embedding_function=embedding_function)\n",
    "    results = db.similarity_search_with_score(query_text, k=5)\n",
    "    pairs = [(query_text, doc.page_content) for doc, _ in results]\n",
    "    scores = reranker.predict(pairs)\n",
    "    reranked = sorted(zip(results, scores), key=lambda x: x[1], reverse=True)\n",
    "    top_docs = [doc for (doc, _), _ in reranked]\n",
    "    annotated_chunks = [f\"[Source: {doc.metadata.get('source', 'unknown')}]\\n{doc.page_content}\" for doc in top_docs]\n",
    "    context_text = \"\\n\\n---\\n\\n\".join(annotated_chunks)\n",
    "\n",
    "    # --- THIS IS THE PLACEHOLDER ---\n",
    "    prompt_text = \"REPLACE_WITH_YOUR_PROMPT_TEMPLATE_STRING\"\n",
    "    prompt_template = ChatPromptTemplate.from_template(prompt_text)\n",
    "    # ------------------------------\n",
    "\n",
    "    inputs = {\"context\": context_text, \"question\": query_text, \"no_of_questions\": no_of_questions}\n",
    "    llm = ChatGoogleGenerativeAI(model=model, temperature=temp, google_api_key=googleapikey)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template, output_parser=StrOutputParser())\n",
    "    response = chain.run(inputs)\n",
    "    final_output_path = f\"{output_path}{subject}_Chapter{chapter}_{query_text}.json\"\n",
    "    # You will need to re-implement the parse_questions function or copy it from your original notebook\n",
    "    # parse_questions(response, no_of_questions, final_output_path) \n",
    "    print(f\"Response from LLM: {response}\")\n",
    "    return f\"Successfully generated {no_of_questions} questions. JSON parsing needs to be re-enabled.\"\n",
    "\n",
    "rag_tool = Tool(\n",
    "    name=\"AdvancedRAGQuestionGenerator\",\n",
    "    func=query_rag_tool,\n",
    "    description=\"Generates academic questions. Input must be a string with format: query_text|no_of_questions|output_path|subject|chapter\"\n",
    ")\n",
    "\n",
    "tools = [rag_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create and Run the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "llm = ChatGoogleGenerativeAI(model=model, temperature=temp, google_api_key=googleapikey)\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "tool_input = \"Nature of software|2|C:/Users/dhili/Desktop/SRIP/week3/Agentic_RAG/|SoftwareEngineering|1\"\n",
    "agent_executor.invoke({\"input\": f\"Use the AdvancedRAGQuestionGenerator tool with the following input: {tool_input}\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}